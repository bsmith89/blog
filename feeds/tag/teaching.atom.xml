<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Deep Ecology - teaching</title><link href="//blog.byronjsmith.com/" rel="alternate"></link><link href="//blog.byronjsmith.com/feeds/tag/teaching.atom.xml" rel="self"></link><id>//blog.byronjsmith.com/</id><updated>2021-01-21T12:00:00-05:00</updated><subtitle>A blog of the new microbiology.</subtitle><entry><title>Things I'm Glad I Learned</title><link href="//blog.byronjsmith.com/things-im-glad-i-learned.html" rel="alternate"></link><published>2021-01-18T18:00:00-05:00</published><updated>2021-01-21T12:00:00-05:00</updated><author><name>Byron J. Smith</name></author><id>tag:blog.byronjsmith.com,2021-01-18:/things-im-glad-i-learned.html</id><summary type="html">&lt;p&gt;&lt;em&gt;WARNING: This post was written with haste and therefore
contains all kinds of typos, spelling errors, grammatical issues, and
delusions of grandeur, wisdom, and writing ability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This post is intended as a living document&amp;mdash;a
gratitude journal of sorts&amp;mdash;of some things that I'm glad I learned.
I expect many of the items on this list will be relevant to computation
biology, but that may change in the future.&lt;/p&gt;
&lt;p&gt;The big idea is that for every item on this list I am (A) glad that
someone introduced me to it,
and (B) think more people should know about it.
This post is my chance to "pay it backwards", as it were;
maybe someone else will be grateful for something they find for the first time
on this list.&lt;/p&gt;
&lt;p&gt;It may also double as an inspiration list for future posts.&lt;/p&gt;
&lt;p&gt;My goal is to write a small blurb for each item â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;WARNING: This post was written with haste and therefore
contains all kinds of typos, spelling errors, grammatical issues, and
delusions of grandeur, wisdom, and writing ability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This post is intended as a living document&amp;mdash;a
gratitude journal of sorts&amp;mdash;of some things that I'm glad I learned.
I expect many of the items on this list will be relevant to computation
biology, but that may change in the future.&lt;/p&gt;
&lt;p&gt;The big idea is that for every item on this list I am (A) glad that
someone introduced me to it,
and (B) think more people should know about it.
This post is my chance to "pay it backwards", as it were;
maybe someone else will be grateful for something they find for the first time
on this list.&lt;/p&gt;
&lt;p&gt;It may also double as an inspiration list for future posts.&lt;/p&gt;
&lt;p&gt;My goal is to write a small blurb for each item, explaining what it is,
how it connects to other items, why I'm grateful for it,
when, and why I first learned about it.
I may add additional links in future edits.
Beyond that, I'll likely leave further discovery up to the reader.&lt;/p&gt;
&lt;h2&gt;Computing&lt;/h2&gt;
&lt;h4&gt;Dependency Graphs (e.g. GNU Make, Snakemake)&lt;/h4&gt;
&lt;p&gt;Many computer people have used GNU Make (or one of its variants) to build
software, but not everyone realizes that it is actually a much more generally
applicable tool.
Make's core idea is that every file that you want to construct requires zero
or more other files as prerequisites.
When you have multiple files, and some of them are prerequisites of
each other, you end up with a directed, acyclic graph describing all of the
chains of dependency.
This description is encoded in Make's domain specific language (DSL) in a
&lt;code&gt;Makefile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The beauty of this dependency graph is that it directly implies optimal
procedures for a whole bunch of useful operations.
First, and most useful, for any target file and set of existing files, Make can
determine the sequence of file constructions that will get from A to B.
As a corollary, it can also determine what &lt;em&gt;doesn't&lt;/em&gt; need to be built
given what already exists.
If you tack on instructions for building each file from its prerequisites, Make
can run those procedures for you automatically.
And if you have timestamps for each file, it can update files only when they're
out of date.&lt;/p&gt;
&lt;p&gt;While this was originally designed for software compilation, it works just
as well for data workflows (or building static HTML for a blog...)&lt;/p&gt;
&lt;p&gt;Since discovering the power of the dependency graph representation and tooling,
I've since upgraded to Snakemake, which fills some additional gaps that come up
in bioinformatics, but is exactly the same concept.
At this point, the Snakefile is the core of all of my (computational) research
projects, and provides me with a version-controlled, and highly modular,
description of all of the steps in my project.&lt;/p&gt;
&lt;p&gt;I think I need to thank C. Titus Brown for introducing me to Make, probably
in summer of 2011 while I was rotating in his lab (but maybe later).&lt;/p&gt;
&lt;h4&gt;Probabilistic Programming (e.g. PyMC3, Pyro)&lt;/h4&gt;
&lt;p&gt;Since first learning about model based inference, at ELME in 2012,
I've been enthralled with frameworks that allow me to take full advantage
of the modularity and flexibility of graphical models without
programming the core algorithms from scratch.
It was actually the process of learning PyMC3 that taught me almost everything
I now know about Bayesian statistics (with a foundation of key concepts
imparted by many other teachers along the way).
Having access to such a powerful DSL makes graphical modeling the hammer for
which I now see an abundance of nails.&lt;/p&gt;
&lt;p&gt;More recently I've started to use Pyro (a different probabilistic programming
framework) as well.
While it was a chore to reshape my mental model of probabilistic programming
with a whole new ecosystem, that has undoubtedly been a valuable learning
experience, and the incredibly fast, GPU accelerated, stochastic gradient
descent algorithms for MAP estimation have been a breakthrough for my research.&lt;/p&gt;
&lt;p&gt;I honestly have no idea who first introduced me to PP, but based on my
GitHub I was teaching myself PyMC3 starting in 2016.&lt;/p&gt;
&lt;h4&gt;Containers (e.g. Docker, Singularity)&lt;/h4&gt;
&lt;p&gt;The newest item on this list (as of December 2020)!&lt;/p&gt;
&lt;p&gt;Containers take what Conda promised (encapsulated, reproducible
software environments), and actually deliver it.
I think the Docker/Singularity combo may have finally solved all of my
software installation problems... Lol.&lt;/p&gt;
&lt;p&gt;This is a hard one to assign thanks, because containerization has been
marketed to me by probably five different people since 2014 or so.
Very grateful that I finally picked it up in 2020!
Should I also thank the incredibly frustrating experience of trying to install
PyMC3/Theano (on a slightly outdated CentOS server, repeatedly, every 6 months)
for finally convincing me to try it out?&lt;/p&gt;
&lt;h4&gt;Command-line / UNIX&lt;/h4&gt;
&lt;p&gt;I'm thankful that I have a glue environment, the UNIX command-line,
for all of my computational work.
Working with a command-line interace (CLI) is foundational for all of the
other headings in this "Computing" section.
I'm (as of now) not including Vim as a separate item, but it really is the
combination of Bash and (Neo)Vim that I'm most grateful for.&lt;/p&gt;
&lt;p&gt;I'm not sure who to thank for this one. Maybe Google, because they sent me a
free Chromebook in the early days (which I used extensively for college
work until the wi-fi broke...).
It was hacking around on that Chromebook's terminal that got me into Linux,
and ultimately resulted in me replacing Windows with Ubuntu on my main laptop.&lt;/p&gt;
&lt;p&gt;I probably wouldn't be the scientist that I am now if it weren't for that
experience.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://xkcd.com/519/"&gt;Relevant XKCD&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Python&lt;/h4&gt;
&lt;p&gt;Anything that I'm capable of programming, I'm capable of programming in Python.&lt;/p&gt;
&lt;p&gt;I'm glad that I know at least one language very well.
I think it makes me a
better teacher (of Python, but also of other languages).
I think it makes
it easier for me to learn new languages and programming concepts.
And I almost never need to decide what language to work in when I'm
doing something complicated; it's always Python.&lt;/p&gt;
&lt;p&gt;Thanks for introducing me to Python goes to Dr. John Hayes, a postdoc
(neuroscientist) I worked with briefly while I was doing undergraduate research
in Margaret Saha's Lab at The College of William &amp;amp; Mary.&lt;/p&gt;
&lt;h4&gt;Functional Programming&lt;/h4&gt;
&lt;p&gt;I'm a Haskell beginner, maybe intermediate depending on who you ask.
I don't do any of my work in Haskell, although I sometimes look for
opportunites.
On the other hand, almost &lt;em&gt;all&lt;/em&gt; of my programming is in a functional style.
I think (without any empirical evidence) I've probably saved myself a lot of
problems by taking pains to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Never reuse variable names for different things&lt;/li&gt;
&lt;li&gt;Avoid mutation ("in the large")&lt;/li&gt;
&lt;li&gt;Use higher-level functions and composition instead of conditionals to control
  program flow&lt;/li&gt;
&lt;li&gt;Keep side-effects away from core logic&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Plain-text&lt;/h4&gt;
&lt;p&gt;Simple text-files and a smattering of UNIX tools will take you 2/3rds to
being a bioinformatician.
Text files are the ultimate form of portability and I believe they should be
used until there is a good reason not to.&lt;/p&gt;
&lt;p&gt;I think Software Carpentry is probably responsible for my positive relationship
with plain-text.&lt;/p&gt;
&lt;h4&gt;Version Control (e.g. Git)&lt;/h4&gt;
&lt;p&gt;If most of your work is plain text, you also get to use version control for
tracking your progress, archiving, sharing, collaboration, and fancy
branching that sometimes comes in handy.&lt;/p&gt;
&lt;p&gt;I am very thankful for having been introduced to Git by Software Carpentry.
It has saved my ass many times.&lt;/p&gt;
&lt;h4&gt;Jupyter notebooks&lt;/h4&gt;
&lt;p&gt;Jupyter Notebooks (and their kin) are great, but only for prototyping and
interactive work; never for reproducible automation.
Notebooks are a core part of my workflow, and the exploratory data analysis
phase of my work is very much done in the Jupyter environment.
On the other hand, my love of workflow managers like Snakemake means that
I try to keep interactive work at the "tips" of my data processing DAG.
Notebooks are a great place to do prototyping, but the second I need the
resulting data for a downstream step it's probably time to convert it to a
script.&lt;/p&gt;
&lt;p&gt;I was introduced to IPython by Greg Wilson and IPython (now Jupyter) Notebooks
by C. Titus Brown during my first Software Carpentry class in 2012.
I'm thankful for all of the work that has gone into making Jupyter Notebooks
the phenomenal tool that they are.&lt;/p&gt;
&lt;h2&gt;Biology&lt;/h2&gt;
&lt;h4&gt;Organic Chemistry&lt;/h4&gt;
&lt;p&gt;I think having first-principles knowledge about how biology is built out of
chemistry has served me very well.&lt;/p&gt;
&lt;p&gt;Not sure there's much more for me to say than that.&lt;/p&gt;
&lt;h4&gt;Neutral Evolution / Spandrils&lt;/h4&gt;
&lt;p&gt;Not everything is adaptive (e.g. I'm 99% sure eye color doesn't affect survival
or reproduction.)
Not everything that seems adaptive, even when there's evidence for convergent
evolution, actually is.
It's incredibly infuriating to see so many Appeals to Natural Selection
(a new logical fallacy I'm naming here first, 2021-01-18) in our
general discourse.&lt;/p&gt;
&lt;p&gt;Some traits are due to random events (neutrality).
Some traits are a natural consequence of physical reality (spandrils).
I'd guess these are important parts of a full answer to a key question in my
field: "why do animals have such fancy microbiomes?"&lt;/p&gt;
&lt;p&gt;If I had to guess, I learned about both neutral evolution and spandrils first
in my evolutionary biology class in college.
I'm sure I can figure out who taught that class if I dig into my emails,
but I'm going to put that off for another day.&lt;/p&gt;
&lt;h4&gt;Microbial Communities&lt;/h4&gt;
&lt;p&gt;I have no doubt that the research I did in high school with Dr. May Voytek at
the USGS (in 2006 and 2007) was one of the most important experiences for my
career development.
I was enthralled then&amp;mdash;as I am now&amp;mdash;with the ways in which microbial
metabolisms can combine in complex communities to perform immensely important
processes.
Microbiomes are a great example of an emergence, and I find that very exciting.&lt;/p&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;h4&gt;Linear Algebra&lt;/h4&gt;
&lt;p&gt;It's true what they say. Linear Algebra is the core of statistic(al computing).&lt;/p&gt;
&lt;p&gt;I learned linear algebra, in a course by that name, my sophomore year of
college,
and have taken a number of courses since then that improved my competence
markedly.
These include mathematical biology classes at W&amp;amp;M (2010?),
graduate level statistics courses
(2012?), and a class on matrix population models in ecology taught by Hal
Caswell (2012).&lt;/p&gt;
&lt;h4&gt;Graphical Models&lt;/h4&gt;
&lt;p&gt;My introduction to graphical models were back-to-back one-week workshop on
maximum likelihood estimation and structural equation modeling taught by Colin
Kremer and Don Schoolmaster, respectively, during two sessions of ELME
("Enhanced Linkages between Mathematics and Ecology") at Kellogg Biological
Station in the summer of 2012.&lt;/p&gt;
&lt;p&gt;Modularizing probability into graphs of relationships between latent
and observed variables is incredibly powerful and is the basis for the
entirety of my statistical understanding.&lt;/p&gt;
&lt;h4&gt;Bayesian Statistics&lt;/h4&gt;
&lt;p&gt;It's not the elegance of Bayes's Rule that I'm enamoured with;
it's how Bayesian statistics &lt;em&gt;democratizes&lt;/em&gt; inference (e.g. the Inference
Button).
If you have a data generating model and some data, you can make
logically consistent statements about it's parameters.
&lt;em&gt;And&lt;/em&gt; you get propagation of uncertainty for free!
How cool is that??&lt;/p&gt;
&lt;p&gt;The first material impact on my understanding of Bayesian stats
was probably in Statistics for Ecologists (was that the class name?),
taught by Dr. Ian Dworkin at Michigan State University.
ELME also played a key role in priming me to understand what Dr. Dworkin was
teaching, although I still only got a whiff of the importance.
It was probably another 4+ years before I learned enough basics to
start calling myself a Bayesian by self-teaching PyMC3.&lt;/p&gt;
&lt;h4&gt;Causal Inference&lt;/h4&gt;
&lt;p&gt;I am by no means fully competent with causal inference, but I aspire to be.&lt;/p&gt;
&lt;p&gt;The idea that we can automate the scientific method
(&lt;code&gt;observation -&amp;gt; hypothesis -&amp;gt; experiment&lt;/code&gt; and all its variations), has been
super influential on the way I think about my work.&lt;/p&gt;
&lt;p&gt;I'd like to do more of this in the coming data.&lt;/p&gt;
&lt;h2&gt;Teaching&lt;/h2&gt;
&lt;h4&gt;Learning Objectives / Concept Inventories&lt;/h4&gt;
&lt;p&gt;It's like the truism that I don't know how to attribute: "The most important
part being able to play a song, is knowing how it goes".
The most important part of teaching is knowing what you are teaching.&lt;/p&gt;
&lt;p&gt;I think the biggest mistake that I've made (and seen made) as a teacher
is not knowing what I'm trying to teach.
Explicit and carefully vetted learning objectives, especially those written
using best practices
(i.e. "At the conclusion of this activity, participants will be able to...")
are a key tool to solve this problem.
Following that with simple questions that (A) assess whether those objectives
have been met, and (B) identify exactly how students' understanding is lacking,
is the real win.
Collections of those questions, called "concept inventories", can (and should
be) shared as a key component of collaborative lesson development.&lt;/p&gt;
&lt;p&gt;I think I have to thank Greg Wilson and Software Carpentry instructor training
(in 2016?) for opening my eyes to the power of this combination.&lt;/p&gt;
&lt;h4&gt;Expert Bias&lt;/h4&gt;
&lt;p&gt;The more we know about a subject the harder it is for us to empathize with
those just starting to learn it.
(See Cognitive Biases, below)
I see it everywhere and staying self-aware about how it limits my
ability to teach has been really important.&lt;/p&gt;
&lt;p&gt;This is another one I picked up from Greg Wilson and Software Carpentry
training.&lt;/p&gt;
&lt;h4&gt;Learning By Teaching&lt;/h4&gt;
&lt;p&gt;For me (and undoubtedly many other people too), I learn something the best
when I have to teach it.
This served me very well during my undergraduate genetics class when
Nishant Kishore and I would cram the night before a test by me "teaching" him
the material to the extent that I had understood it.&lt;/p&gt;
&lt;p&gt;This compounds with other ideas in pedagogy, like Expert Bias, and I think
its a big part of why the Think-Pair-Share tactic works so well.&lt;/p&gt;
&lt;h2&gt;Life&lt;/h2&gt;
&lt;h4&gt;Growth Mindset&lt;/h4&gt;
&lt;p&gt;It feels incredibly powerful to see your current self as just a single stage
in the development of your full capabilities.&lt;/p&gt;
&lt;p&gt;I think I really first learned this in Multivariate Calculus my sophomore
year; I distinctly remember how gratifying it was to go from absolutely zero
understanding to a B-minus on the final; a bad grade relative to my other
classes but I'm very proud of it nonetheless.&lt;/p&gt;
&lt;p&gt;Knowing that I am a work in progress is both liberating and empowering.&lt;/p&gt;
&lt;h4&gt;Deficit Model of Persuasion&lt;/h4&gt;
&lt;p&gt;People who disagree with you about the way the world works often have all the
same facts as you do.
We don't convince people of our world view by giving them more facts.&lt;/p&gt;
&lt;p&gt;I find this depressing. But on the flip side you &lt;em&gt;can&lt;/em&gt; be much more effective
at persuasion by doing two key things (to my limited understanding):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Share your emotional reasons for your beliefs
  (e.g. I'm super &lt;strong&gt;excited&lt;/strong&gt; to be able to travel and hug my parents again
  after they and I are vaccinated for COVID-19.
  I've been &lt;strong&gt;scared&lt;/strong&gt; about their safety, and vaccination feels like an
  opportunity for &lt;strong&gt;optimism&lt;/strong&gt;.)&lt;/li&gt;
&lt;li&gt;Demonstrate that their community shares that belief.
  This is usually done by making them part of &lt;em&gt;your&lt;/em&gt; community.
  (I think it's really great that &lt;strong&gt;our&lt;/strong&gt; family/college/neighborhood has
  really come together to get everyone vaccinated.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is obviously key for science communication.&lt;/p&gt;
&lt;h4&gt;Privelege&lt;/h4&gt;
&lt;p&gt;Recognizing my advantages has been really helpful in building empathy for
disadvantaged groups in the world.
I'd like everyone to have the same access and opportunities for success and
contribution that I have had.&lt;/p&gt;
&lt;p&gt;I probably internalized this sense circa 2015...ish.
Not sure if there's any one person to thank.&lt;/p&gt;
&lt;h4&gt;Talking About Yourself (e.g. "I feel..." Language)&lt;/h4&gt;
&lt;p&gt;I think that my interpersonal relationships have been bolstered by
a mutual ability to communicate about our own feelings instead
of claiming some objective sense of reality ("I am... You are...").&lt;/p&gt;
&lt;p&gt;I can say with some certainty that communication is much more likely to
transition into a fight when I break this rule.&lt;/p&gt;
&lt;p&gt;I realize this is common advice, but it has served me well.&lt;/p&gt;
&lt;h4&gt;Cognitive Biases&lt;/h4&gt;
&lt;p&gt;Especially the "Fundamental Attribution Error".&lt;/p&gt;
&lt;p&gt;Mostly because I am also subject to all of the same biases, but also
because it helps me to understand peoples' behavior, even at large scales.&lt;/p&gt;
&lt;h4&gt;Profit and Monopoly&lt;/h4&gt;
&lt;p&gt;I think I have a hobbyist's understanding of economic theory.
As I understand it, in an idealized world (perfect market efficiency) no
producer should be able to make a marginal profit in the long run.
Obviously this is not really the case, since e.g. doctors are making
great money for their time.
I think it's fascinating that in many (most?) cases where value is being
captured by a small group of people this can be assigned to some sort of
monopoly power.
For instance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;doctors have the American Medical Association controlling the supply of
  medical labor,&lt;/li&gt;
&lt;li&gt;Genentech has various patents and corporate secrets, and&lt;/li&gt;
&lt;li&gt;ISPs have both cartel power and huge barriers to entry in most regions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Monopolies aren't all evil (left as an exercise to the reader).
Appropriately regulating these monopolies seems to me to be a key
role of governments.&lt;/p&gt;
&lt;h4&gt;Intentional Communities&lt;/h4&gt;
&lt;p&gt;In this millennium, many people (including me) are suffering from less community
than the previous one.
This would appear to be due to geographic mobility (loss of family and
friends), secularization, an extended youth, etc.&lt;/p&gt;
&lt;p&gt;I think it has been empowering to realize that we can choose to have those
communities if we're willing to work for it.
Intentional communities (e.g. communes, kibbutzim) prove it, and give me
something to aspire to.&lt;/p&gt;
&lt;h4&gt;Getting Things Done (GTD)&lt;/h4&gt;
&lt;p&gt;I don't actually use the GTD system, so why is this one on the list?
For me, GTD taught me to use to-do lists as a plan for the day, rather than
(only) record-keeping about what I need to do.
Specifically GTD introduced me to the tactic of giving myself a short list of
&lt;em&gt;only&lt;/em&gt; the tasks that are currently a priority, and using my own behavior to
re-assess if things are &lt;em&gt;actually&lt;/em&gt; important.&lt;/p&gt;
&lt;p&gt;To-do lists can serve other purposes as well, but motivation and prioritization
are two key ones for me.&lt;/p&gt;
&lt;h4&gt;Habits&lt;/h4&gt;
&lt;p&gt;I very much believe that the most important factor in whether or not we
demonstrating self-control in our lives is the degree to which we actually
have to flex that muscle.
Habits are an important way that we can unload the weight of "I should really
do X" while &lt;em&gt;still doing X&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;Rituals&lt;/h4&gt;
&lt;p&gt;For me at least, lots of the stress in life comes from worrying or thinking
about things in a non-productive way or over which I don't have control.
Rituals have been valuable for me in putting clear landmarks in my
day/week/year that allow me to reset my mental state.
Many of them are also just enjoyable on their own.
Some key rituals that come to mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Daily:&lt;ul&gt;
&lt;li&gt;Reading the news as I wake up&lt;/li&gt;
&lt;li&gt;Making/drinking coffee in the morning&lt;/li&gt;
&lt;li&gt;Ending the core of my workday with a beer/cocktail&lt;/li&gt;
&lt;li&gt;Washing my hands thoroughly when getting to work, home&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Week:&lt;ul&gt;
&lt;li&gt;Going for a run/walk without a schedule or destination&lt;/li&gt;
&lt;li&gt;Cooking an expansive dinner&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Yearly&lt;ul&gt;
&lt;li&gt;Traveling to be with my family&lt;/li&gt;
&lt;li&gt;Deep-cleaning my home&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I still want to be intentional about building more of these into my life.&lt;/p&gt;</content><category term="Misc."></category><category term="software-carpentry"></category><category term="teaching"></category><category term="python"></category><category term="programming"></category><category term="open science"></category><category term="microbiome"></category><category term="rant"></category><category term="evolution"></category><category term="pipelines"></category><category term="software"></category><category term="git"></category><category term="containers"></category></entry><entry><title>Tutorial: Reproducible data analysis pipelines using Snakemake</title><link href="//blog.byronjsmith.com/snakemake-analysis.html" rel="alternate"></link><published>2017-11-19T17:00:00-05:00</published><updated>2017-11-19T17:00:00-05:00</updated><author><name>Byron J. Smith</name></author><id>tag:blog.byronjsmith.com,2017-11-19:/snakemake-analysis.html</id><summary type="html">&lt;p&gt;In many areas of natural and social science, as well as engineering, data
analysis involves a series of transformations: filtering, aggregating,
comparing to theoretical models, culminating in the visualization and
communication of results.
This process is rarely static, however, and
components of the analysis pipeline are frequently subject to replacement
and refinement, resulting in challenges for reproducing computational
results.
Describing data analysis as a directed network of transformations
has proven useful for translating between human intuition and computer
automation.
In the past I've &lt;a href="//blog.byronjsmith.com/makefile-shortcuts.html"&gt;evangelized extensively for GNU Make&lt;/a&gt;,
which takes advantage of this graph representation to enable incremental builds
and parallelization.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Snakemake&lt;/em&gt; is a next-generation tool based on this concept and designed
specifically for bioinformatics and other complex, computationally
challenging analyses.
I've started using &lt;em&gt;Snakemake&lt;/em&gt; for my own data analysis projects, and I've
found it to be a consistent improvement, enabling more complex pipelines with
fewer of the "hacks" that â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In many areas of natural and social science, as well as engineering, data
analysis involves a series of transformations: filtering, aggregating,
comparing to theoretical models, culminating in the visualization and
communication of results.
This process is rarely static, however, and
components of the analysis pipeline are frequently subject to replacement
and refinement, resulting in challenges for reproducing computational
results.
Describing data analysis as a directed network of transformations
has proven useful for translating between human intuition and computer
automation.
In the past I've &lt;a href="//blog.byronjsmith.com/makefile-shortcuts.html"&gt;evangelized extensively for GNU Make&lt;/a&gt;,
which takes advantage of this graph representation to enable incremental builds
and parallelization.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Snakemake&lt;/em&gt; is a next-generation tool based on this concept and designed
specifically for bioinformatics and other complex, computationally
challenging analyses.
I've started using &lt;em&gt;Snakemake&lt;/em&gt; for my own data analysis projects, and I've
found it to be a consistent improvement, enabling more complex pipelines with
fewer of the "hacks" that are often necessary when using &lt;em&gt;Make&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I've taught &lt;a href="//blog.byronjsmith.com/make-analysis.html"&gt;&lt;em&gt;Make&lt;/em&gt; workshops in the past&lt;/a&gt;,
so, when I was invited to present to the Boise State University Joint
User Groups, I was excited to convert that tutorial to &lt;em&gt;Snakemake&lt;/em&gt;.
Here, I've converted that tutorial into a blog post.
The original (and therefore this lesson as well) is inspired by the
&lt;a href="https://swcarpentry.github.io/make-novice/"&gt;Software Carpentry &lt;em&gt;Make&lt;/em&gt; lesson&lt;/a&gt;,
to which I am also a contributor.&lt;/p&gt;
&lt;p&gt;Some prior experience with the command line is assumed, and learners are
encouraged to follow along on their own computers.
The entire tutorial, including questions for the learner are designed to
take 2 hours as a live-coded, Software Carpentry style lesson.
A standalone lesson repository can be found &lt;a href="https://github.com/bsmith89/snakemake-boise"&gt;here&lt;/a&gt; and is
licensed CC-BY.&lt;/p&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;p&gt;If not already on your computer, install the following prerequistes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;em&gt;Bash&lt;/em&gt; terminal&lt;/li&gt;
&lt;li&gt;Python 3.6 and the following packages&lt;ul&gt;
&lt;li&gt;&lt;code&gt;snakemake&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matplotlib&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The lesson assumes the following programs are also installed, but
    they're not absolutely necessary for the flow of the lesson,
    and/or alternatives are widely available:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;head&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nano&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tree&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/bsmith89/zipf-example"&gt;This example directory&lt;/a&gt; should be downloaded to the user's
desktop and navigated into at the command line.
(e.g. &lt;code&gt;git clone https://github.com/bsmith89/zipf-example; cd zipf-example&lt;/code&gt;)&lt;/p&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;h2&gt;Zipf's Law [10 minutes]&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The most frequently-occurring word occurs approximately twice as
often as the second most frequent word. This is
&lt;a href="http://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Zipf's Law&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's imagine that we're interested in testing Zipf's law in some of our
favorite books.
We've compiled our raw data: the books we want to analyze,
and have prepared several Python scripts that together make up our
analysis pipeline.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;tree&lt;/code&gt; command produces a handy tree-diagram of the directory.
(You may not have this program installed on your computer.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;analysis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;Â Â  &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;LICENSE_TEXTS&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;Â Â  &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;Â Â  &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;Â Â  &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="k"&gt;last&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;Â Â  &lt;span class="err"&gt;â””â”€â”€&lt;/span&gt; &lt;span class="n"&gt;sierra&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;matplotlibrc&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pip&lt;/span&gt;
&lt;span class="err"&gt;â””â”€â”€&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;
    &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;plotcount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
    &lt;span class="err"&gt;â””â”€â”€&lt;/span&gt; &lt;span class="n"&gt;wordcount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;

&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;directories&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here you see that we're starting with a well designed project directory.
The raw data (books) are stored in their own directory, and scripts have
informative names.&lt;/p&gt;
&lt;p&gt;Let's take a look at our raw data&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head books/isles.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Our first step is to count the frequency of each word in a book.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;scripts/wordcount.py books/isles.txt isles.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's take a quick peek at the result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head -5 isles.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;shows us the top 5 lines in the output file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;the 3822    6.7371760973&lt;/span&gt;
&lt;span class="err"&gt;of  2460    4.33632998414&lt;/span&gt;
&lt;span class="err"&gt;and 1723    3.03719372466&lt;/span&gt;
&lt;span class="err"&gt;to  1479    2.60708619778&lt;/span&gt;
&lt;span class="err"&gt;a   1308    2.30565838181&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each row shows the word itself, the number of occurrences of that
word, and the number of occurrences as a percentage of the total
number of words in the text file.&lt;/p&gt;
&lt;p&gt;We can do the same thing for a different book:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;scripts/wordcount.py books/abyss.txt abyss.tsv
head -5 abyss.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, let's visualize the results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;scripts/plotcount.py isles.tsv ascii
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ascii&lt;/code&gt; argument has been added so that we get a text-based
bar-plot printed to the screen.&lt;/p&gt;
&lt;p&gt;The script is also able to render a graphical bar-plot using matplotlib
and save the figure to a named file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;scripts/plotcount.py isles.tsv isles.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Together these scripts implement a common workflow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read a data file.&lt;/li&gt;
&lt;li&gt;Perform an analysis on this data file.&lt;/li&gt;
&lt;li&gt;Write the analysis results to a new file.&lt;/li&gt;
&lt;li&gt;Plot a graph of the analysis results.&lt;/li&gt;
&lt;li&gt;Save the graph as an image, so we can publish it.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Writing a script to do our analysis [5 minutes]&lt;/h2&gt;
&lt;p&gt;Running this pipeline for one book is relatively simple using the command-line.
But once the number of files and the number of steps in the pipeline
expands, this can turn into a lot of work.
Plus, no one wants to sit and wait for a command to finish, even just for 30
seconds.&lt;/p&gt;
&lt;p&gt;The most common solution to the tedium of data processing is to write
a master script that runs the whole pipeline from start to finish.&lt;/p&gt;
&lt;p&gt;We can see such a script in &lt;code&gt;analysis.sh&lt;/code&gt;, which contains:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class="c1"&gt;# USAGE: bash analysis.sh&lt;/span&gt;
&lt;span class="c1"&gt;# to produce plots for isles and abyss.&lt;/span&gt;

scripts/wordcount.py books/isles.txt isles.tsv
scripts/wordcount.py books/abyss.txt abyss.tsv

scripts/plotcount.py isles.tsv isles.png
scripts/plotcount.py abyss.tsv abyss.png

&lt;span class="c1"&gt;# Archive the results.&lt;/span&gt;
rm -rf zipf_results
mkdir zipf_results
cp isles.tsv abyss.tsv isles.png abyss.png zipf_results/
tar -czf zipf_results.tgz zipf_results
rm -r zipf_results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This master script solved several problems in computational reproducibility:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It explicitly documents our pipeline,
    making communication with colleagues (and our future selves) more efficient.&lt;/li&gt;
&lt;li&gt;It allows us to type a single command, &lt;code&gt;bash analysis.sh&lt;/code&gt;, to
    reproduce the full analysis.&lt;/li&gt;
&lt;li&gt;It prevents us from &lt;em&gt;repeating&lt;/em&gt; typos or mistakes.
    You might not get it right the first time, but once you fix something
    it'll (probably) stay that way.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;What are the problems with this approach? [10 minutes]&lt;/h2&gt;
&lt;p&gt;A master script is a good start, but it has a few shortcomings.&lt;/p&gt;
&lt;p&gt;Let's imagine that we adjusted the width of the bars in our plot
by editing &lt;code&gt;scripts/plotcount.py&lt;/code&gt;;
in the function definition for
&lt;code&gt;plot_word_counts&lt;/code&gt;, &lt;code&gt;width = 1.0&lt;/code&gt; is now &lt;code&gt;width = 0.8&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we want to recreate our figures.
We &lt;em&gt;could&lt;/em&gt; &lt;code&gt;bash analysis.sh&lt;/code&gt; again.
That would work, but it could also be a big pain if counting words takes
more than a few seconds.
The word counting routine hasn't changed; we shouldn't need to recreate
those files.&lt;/p&gt;
&lt;p&gt;Alternatively, we could manually rerun the plotting for each word-count file
and recreate the archive.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; file in *.tsv&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    scripts/plotcount.py &lt;span class="nv"&gt;$file&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;file&lt;/span&gt;&lt;span class="p"&gt;/.tsv/.png&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;

rm -rf zipf_results
mkdir zipf_results
cp isles.tsv abyss.tsv isles.png abyss.png zipf_results/
tar -czf zipf_results.tgz zipf_results
rm -r zipf_results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But by then we've nullified many of the benefits of having a master script in
the first place.&lt;/p&gt;
&lt;p&gt;Another popular option is to comment out a subset of the lines in
&lt;code&gt;analysis.sh&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class="c1"&gt;# USAGE: bash analysis.sh&lt;/span&gt;
&lt;span class="c1"&gt;# to produce plots for isles and abyss.&lt;/span&gt;

&lt;span class="c1"&gt;# These lines are commented out because they don&amp;#39;t need to be rerun.&lt;/span&gt;
&lt;span class="c1"&gt;#scripts/wordcount.py isles.txt isles.tsv&lt;/span&gt;
&lt;span class="c1"&gt;#scripts/wordcount.py abyss.txt abyss.tsv&lt;/span&gt;

scripts/plotcount.py isles.tsv isles.png
scripts/plotcount.py abyss.tsv abyss.png

&lt;span class="c1"&gt;# Archive the results.&lt;/span&gt;
rm -rf zipf_results
mkdir zipf_results
cp isles.tsv abyss.tsv isles.png abyss.png zipf_results/
tar -czf zipf_results.tgz zipf_results
rm -r zipf_results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Followed by &lt;code&gt;bash analysis.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But this process, and subsequently undoing it,
can be a hassle and source of errors in complicated pipelines.&lt;/p&gt;
&lt;p&gt;What we really want is an executable &lt;em&gt;description&lt;/em&gt; of our pipeline that
allows software to do the tricky part for us:
figuring out what steps need to be rerun.
It would also be nice if this tool encourage a &lt;em&gt;modular&lt;/em&gt; analysis
and reusing instead of rewriting parts of our pipeline.
As an added benefit, we'd like it all to play nice with the other
mainstays of reproducible research: version control, Unix-style tools,
and a variety of scripting languages.&lt;/p&gt;
&lt;h1&gt;Snakemake background [5 minutes]&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Snakemake&lt;/em&gt; comes from a lineage of computer programs&amp;mdash;most notably
 &lt;em&gt;Make&lt;/em&gt;&amp;mdash;originally designed to
automate the compilation and installation of software.
Programs like &lt;em&gt;Make&lt;/em&gt; automate the building of target files through a series of
discrete steps.
Despite the original purpose, this design makes it a great fit for
bioinformatics pipelines, which usually work by transforming data from one form
to another
(e.g. &lt;em&gt;raw data&lt;/em&gt; &amp;#8594; &lt;em&gt;word counts&lt;/em&gt; &amp;#8594; &lt;em&gt;???&lt;/em&gt; &amp;#8594; &lt;em&gt;profit&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Snakemake&lt;/em&gt; is inspired by this approach, but designed specifically for
computationally intensive and/or complex data analysis pipelines.
The name is a reference to the programming language &lt;em&gt;Python&lt;/em&gt;, which forms
the basis for the &lt;em&gt;Snakemake&lt;/em&gt; syntax.
You don't need to be an expert at &lt;em&gt;Python&lt;/em&gt; to use &lt;em&gt;Snakemake&lt;/em&gt;, but it can
sometimes be very useful.
There are pros and cons to using &lt;em&gt;Snakemake&lt;/em&gt; versus any other analysis pipeline
tools, and it is worth considering other options, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;GNU Make&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;doit&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Galaxy&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Tutorial&lt;/h1&gt;
&lt;h2&gt;Writing and Running Snakefiles [10 minutes]&lt;/h2&gt;
&lt;p&gt;Let's get started writing a description of our analysis for &lt;em&gt;Snakemake&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Open up a file called &lt;code&gt;Snakefile&lt;/code&gt; in your editor and add the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule wordcount_isles:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;books/isles.txt&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;isles.tsv&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;scripts/wordcount.py books/isles.txt isles.tsv&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We have now written the simplest, non-trivial snakefile.
The &lt;code&gt;shell:&lt;/code&gt; line is pretty reminiscent of one of the lines from our master
script.
I bet you can already see what this snakefile means.&lt;/p&gt;
&lt;p&gt;Let's walk through what we've written.
The first line uses the keyword &lt;code&gt;rule&lt;/code&gt; followed by the name of our rule:
&lt;code&gt;wordcount_isles&lt;/code&gt;.
We end that line with a colon.
All of the following lines in our rule are indented with four spaces.
The second line says that it takes an input file, using the &lt;code&gt;input&lt;/code&gt;
keyword which is again followed by a colon.
We then give it the path to this prerequisite (&lt;code&gt;books/isles.txt&lt;/code&gt;), wrapped in
quotes.
The third line does the same thing with the output file (&lt;code&gt;isles.tsv&lt;/code&gt;).
And the last line is the exact shell command that we used in our shell script
earlier to create the target output file.
Like scripting, &lt;em&gt;Snakemake&lt;/em&gt; allows us to wrap a series of shell commands, but
is more expressive and flexible than a script.&lt;/p&gt;
&lt;p&gt;Our snakefile describes a (very short) pipeline:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We are generating a file called &lt;code&gt;isles.tsv&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Creating this file requires &lt;code&gt;books/isles.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The command to create this file runs the script runs &lt;code&gt;wordcount.py&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We'll think about our pipeline as a network of files that are dependent
on one another.
Right now our Snakefile describes a pretty simple &lt;strong&gt;dependency graph&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;books/isles.txt&lt;/code&gt; &amp;#8594; &lt;code&gt;isles.tsv&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where the "&amp;#8594;" is pointing from requirements to targets.&lt;/p&gt;
&lt;h3&gt;Running Snakemake&lt;/h3&gt;
&lt;p&gt;Now that we have a (currently incomplete) description of our pipeline,
let's use &lt;em&gt;Snakemake&lt;/em&gt; to execute it.&lt;/p&gt;
&lt;p&gt;First, remove the previously generated files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;rm *.tsv *.png zipf_results.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;snakemake isles.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see the following print to the terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;Provided&lt;/span&gt; &lt;span class="n"&gt;cores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Rules&lt;/span&gt; &lt;span class="n"&gt;claiming&lt;/span&gt; &lt;span class="k"&gt;more&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="n"&gt;scaled&lt;/span&gt; &lt;span class="n"&gt;down&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Job&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;count&lt;/span&gt;   &lt;span class="n"&gt;jobs&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;wordcount_isles&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;wordcount_isles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;
    &lt;span class="n"&gt;jobid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="n"&gt;Finished&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By default, &lt;em&gt;Snakemake&lt;/em&gt; prints a summary of the recipes that it
executes.&lt;/p&gt;
&lt;p&gt;Let's see if we got what we expected.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head -5 isles.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first 5 lines of that file should look exactly like before.&lt;/p&gt;
&lt;h3&gt;Re-running Snakemake&lt;/h3&gt;
&lt;p&gt;Let's try running &lt;em&gt;Snakemake&lt;/em&gt; the same way again.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;snakemake isles.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This time, instead of executing the same recipe,
&lt;em&gt;Snakemake&lt;/em&gt; prints &lt;code&gt;Nothing to be done.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;What's happening here?&lt;/p&gt;
&lt;p&gt;When you ask &lt;em&gt;Snakemake&lt;/em&gt; to make &lt;code&gt;isles.tsv&lt;/code&gt; it first looks at
the modification time of that target.
Next it looks at the modification time for the target's prerequisites.
If the target is newer than the prerequisites &lt;em&gt;Snakemake&lt;/em&gt; decides that
the target is up-to-date and does not need to be remade.&lt;/p&gt;
&lt;p&gt;Much has been said about using modification times as the cue for remaking
files.
This can be another &lt;em&gt;Snakemake&lt;/em&gt; gotcha, so keep it in mind.&lt;/p&gt;
&lt;p&gt;If you want to induce the original behavior, you only have to
change the modification time of &lt;code&gt;books/isles.txt&lt;/code&gt; so that it is newer
than &lt;code&gt;isles.tsv&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;touch books/isles.txt
snakemake isles.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The original behavior is restored.&lt;/p&gt;
&lt;p&gt;Sometimes you only want &lt;em&gt;Snakemake&lt;/em&gt; to tell you what it thinks about the
current state of your files.
&lt;code&gt;snakemake --dryrun isles.tsv&lt;/code&gt; will print &lt;em&gt;Snakemake&lt;/em&gt;'s execution plan,
without actually carrying it out.
The flag can also be abbreviated as &lt;code&gt;-n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you don't pass a target as an argument to snakemake (i.e. run
&lt;code&gt;snakemake&lt;/code&gt;) it will assume that you want to build the first target in the
snakefile.&lt;/p&gt;
&lt;h2&gt;Expanding our Snakefile with more recipes (and challenge) [20 minutes]&lt;/h2&gt;
&lt;p&gt;Now that &lt;em&gt;Make&lt;/em&gt; knows how to build &lt;code&gt;isles.tsv&lt;/code&gt;,
we can add a rule for plotting those results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule plotcount_isles:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;isles.tsv&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;isles.png&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;scripts/plotcount.py isles.tsv isles.png&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The dependency graph now looks like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;books/isles.txt&lt;/code&gt; &amp;#8594; &lt;code&gt;isles.tsv&lt;/code&gt; &amp;#8594; &lt;code&gt;isles.png&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's add a few more recipes to our Snakefile.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;wordcount_abyss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;books/abyss.txt&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.tsv&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;scripts/wordcount.py books/abyss.txt abyss.tsv&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;archive_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.png&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;zipf_results.tgz&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="ss"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;        rm -rf zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        mkdir zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        cp isles.tsv abyss.tsv isles.png abyss.png zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        tar -czf zipf_results.tgz zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        rm -r zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here the recipe for &lt;code&gt;zipf_results.tgz&lt;/code&gt; takes multiple input files,
each of which must be quoted and separated by commas, and involves
involves running a series of shell commands.
When building the archive, &lt;em&gt;Snakemake&lt;/em&gt; will run each line successively unless
any return an error.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Question&lt;/h4&gt;
&lt;p&gt;Without doing it, what happens if you run &lt;code&gt;snakemake isles.png&lt;/code&gt;?&lt;/p&gt;
&lt;h4&gt;Challenge&lt;/h4&gt;
&lt;p&gt;What does the dependency graph look like for your Snakefile?&lt;/p&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens if you run &lt;code&gt;snakemake zipf_results.tgz&lt;/code&gt; right now?&lt;/p&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Write a recipe for &lt;code&gt;abyss.png&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once you've written a recipe for &lt;code&gt;abyss.png&lt;/code&gt; you should be able to
run &lt;code&gt;snakemake zipf_results.tgz&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's delete all of our files and try it out.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;rm abyss.* isles.*
snakemake zipf_results.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should get the something like the following output
(the order may be different)
to your terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;Provided&lt;/span&gt; &lt;span class="n"&gt;cores&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Rules&lt;/span&gt; &lt;span class="n"&gt;claiming&lt;/span&gt; &lt;span class="k"&gt;more&lt;/span&gt; &lt;span class="n"&gt;threads&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;be&lt;/span&gt; &lt;span class="n"&gt;scaled&lt;/span&gt; &lt;span class="n"&gt;down&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Job&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;count&lt;/span&gt;   &lt;span class="n"&gt;jobs&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;archive_results&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;plotcount_abyss&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;plotcount_isles&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;wordcount_abyss&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="n"&gt;wordcount_isles&lt;/span&gt;
        &lt;span class="mi"&gt;5&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;wordcount_abyss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;
    &lt;span class="n"&gt;jobid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;Finished&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;wordcount_isles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;
    &lt;span class="n"&gt;jobid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;

&lt;span class="n"&gt;Finished&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;plotcount_abyss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;png&lt;/span&gt;
    &lt;span class="n"&gt;jobid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="n"&gt;Finished&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;plotcount_isles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;png&lt;/span&gt;
    &lt;span class="n"&gt;jobid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="n"&gt;Finished&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;archive_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tsv&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;png&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;png&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;zipf_results&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tgz&lt;/span&gt;
    &lt;span class="n"&gt;jobid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="n"&gt;Finished&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="k"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;steps&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since you asked for &lt;code&gt;zipf_results.tgz&lt;/code&gt; &lt;em&gt;Snakemake&lt;/em&gt; looked first for that file.
Not finding it, &lt;em&gt;Snakemake&lt;/em&gt; looked for its prerequisites.
Since none of those existed it remade the ones it could,
&lt;code&gt;abyss.tsv&lt;/code&gt; and &lt;code&gt;isles.tsv&lt;/code&gt;.
Once those were finished it was able to make &lt;code&gt;abyss.png&lt;/code&gt; and
&lt;code&gt;isles.png&lt;/code&gt;, before finally building &lt;code&gt;zipf_results.tgz&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens if you &lt;code&gt;touch abyss.tsv&lt;/code&gt; and
then &lt;code&gt;snakemake zipf_results.tgz&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Running Snakemake in parallel&lt;/h2&gt;
&lt;p&gt;And check this out!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;snakemake clean
snakemake --threads &lt;span class="m"&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Did you see it?
The &lt;code&gt;--threads 2&lt;/code&gt; flag (just "&lt;code&gt;-j2&lt;/code&gt;" works too) tells &lt;em&gt;Make&lt;/em&gt; to run recipes in
two &lt;em&gt;parallel&lt;/em&gt; threads.
Our dependency graph clearly shows that
&lt;code&gt;abyss.tsv&lt;/code&gt; and &lt;code&gt;isles.tsv&lt;/code&gt; are mutually independent and can
both be built at the same time.
Likewise for &lt;code&gt;abyss.png&lt;/code&gt; and &lt;code&gt;isles.png&lt;/code&gt;.
If you've got a bunch of independent branches in your analysis, this can
greatly speed up your build process.&lt;/p&gt;
&lt;h3&gt;Phony targets&lt;/h3&gt;
&lt;p&gt;Sometimes we want to build a bunch of different files simultaneously.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule all:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;isles.png&amp;quot;, &amp;quot;abyss.png&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Even though this rule doesn't have a recipe, it does have prerequisites.
Now, when you run &lt;code&gt;snakemake all&lt;/code&gt; &lt;em&gt;Snakemake&lt;/em&gt; will do what it needs to to bring
both of those targets up to date.&lt;/p&gt;
&lt;p&gt;It is traditional for "&lt;code&gt;all&lt;/code&gt;" to be the first recipe in a snakefile,
since the first recipe is what is built by default
when no other target is passed as an argument.&lt;/p&gt;
&lt;p&gt;Another traditional target is "&lt;code&gt;clean&lt;/code&gt;".
Add the following to your snakefile.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule clean:&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;rm --force *.tsv *.png zipf_results.tgz&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code&gt;snakemake clean&lt;/code&gt; will now remove all of the cruft.&lt;/p&gt;
&lt;h2&gt;Diagramming the DAG [5 minutes]&lt;/h2&gt;
&lt;p&gt;(If you'd prefer not to bake this Snakefile from scratch, you can
get one we've been hiding in the oven the whole time:
&lt;code&gt;cp .extra/Snakefile.1 Snakefile&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Right now, our snakefile looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Dummy&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;
&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="k"&gt;all&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.png&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;rm --force *.tsv *.png zipf_results.tgz&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Analysis&lt;/span&gt;
&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;wordcount_isles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;books/isles.txt&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.tsv&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;scripts/wordcount.py books/isles.txt isles.tsv&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;wordcount_abyss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;books/abyss.txt&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.tsv&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;scripts/wordcount.py books/abyss.txt abyss.tsv&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Plotting&lt;/span&gt;
&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;plotcount_isles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.tsv&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.png&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;scripts/plotcount.py isles.tsv isles.png&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;plotcount_abyss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.tsv&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.png&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;scripts/plotcount.py abyss.tsv abyss.png&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Deliverables&lt;/span&gt;
&lt;span class="k"&gt;rule&lt;/span&gt; &lt;span class="n"&gt;archive_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;isles.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;abyss.png&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;zipf_results.tgz&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="ss"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="ss"&gt;        rm -rf zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        mkdir zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        cp isles.tsv abyss.tsv isles.png abyss.png zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        tar -czf zipf_results.tgz zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        rm -r zipf_results/&lt;/span&gt;
&lt;span class="ss"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Looks good, don't you think?
Notice the added comments, starting with the "&lt;code&gt;#&lt;/code&gt;" character just like in
Python, R, shell, etc.&lt;/p&gt;
&lt;p&gt;Using these recipes, a simple call to &lt;code&gt;snakemake&lt;/code&gt; builds all the same files
that we were originally making either manually or using the master script, but
with a few bonus features.&lt;/p&gt;
&lt;p&gt;Now, if we change one of the inputs, we don't have to rebuild everything.
Instead, &lt;em&gt;Snakemake&lt;/em&gt; knows to only rebuild the files that, either directly or
indirectly, depend on the file that changed.
This is called an &lt;strong&gt;incremental build&lt;/strong&gt;.
It's no longer our job to track those dependencies.
One fewer cognitive burden getting in the way of research progress!&lt;/p&gt;
&lt;p&gt;In addition, a snakefile explicitly documents the inputs to and outputs
from every step in the analysis.
These are like informal "USAGE:" documentation for our scripts.&lt;/p&gt;
&lt;p&gt;It is worth pointing out that our pipeline (and every pipeline) &lt;em&gt;must&lt;/em&gt; be
acyclic: no file can be an input to itself or to any of its inputs, &lt;em&gt;ad
infinitum&lt;/em&gt;.
Officially we talk about the relationships between files as a Directed Acyclic
Graph (DAG).
While earlier we took the time to diagram our DAG by hand, &lt;em&gt;Snakemake&lt;/em&gt;
has tools for plotting this network automatically.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;snakemake --dag zipf_results.tgz &lt;span class="p"&gt;|&lt;/span&gt; dot -Tpng &amp;gt; dag.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Open that file and check it out.&lt;/p&gt;
&lt;p&gt;&lt;img alt="A visualization of the analysis DAG" src="//blog.byronjsmith.com/static/images/snakemake-dag.png"&gt;&lt;/p&gt;
&lt;p&gt;Diagrams like this one can be a very useful way to debug problems with an
analysis pipeline.&lt;/p&gt;
&lt;h2&gt;Don't repeat yourself&lt;/h2&gt;
&lt;p&gt;In many programming language, the bulk of the language features are there
to allow the programmer to describe long-winded computational routines as
short, expressive, beautiful code.
Features in Python or R like user-defined variables and functions are
useful in part because they mean we don't have to write out (or think about)
all of the details over and over again.
This good habit of writing things out only once is known as the D.R.Y.
principle.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;Snakemake&lt;/em&gt; a number of features are designed to minimize repetitive code.
Our current snakefile does &lt;em&gt;not&lt;/em&gt; conform to this principle,
but &lt;em&gt;Snakemake&lt;/em&gt; is perfectly capable of doing so.&lt;/p&gt;
&lt;h3&gt;Automatic variables [10 minutes]&lt;/h3&gt;
&lt;blockquote&gt;
&lt;h4&gt;Question&lt;/h4&gt;
&lt;p&gt;What are some of the repetitive components of our snakefile?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One overly repetitive part of our Snakefile:
Inputs and outputs are in both the header &lt;em&gt;and&lt;/em&gt; the recipe of each rule.&lt;/p&gt;
&lt;p&gt;It turns out, that&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule wordcount_isles:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;books/isles.txt&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;isles.tsv&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;scripts/wordcount.py books/isles.txt isles.tsv&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Can be rewritten as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule wordcount_isles:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;books/isles.txt&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;isles.tsv&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;scripts/wordcount.py {input} {output}&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we've replaced the input "&lt;code&gt;books/isles.txt&lt;/code&gt;" in the recipe
with "&lt;code&gt;{input}&lt;/code&gt;" and the output "&lt;code&gt;isles.dat&lt;/code&gt;" with "&lt;code&gt;{output}&lt;/code&gt;".
Both "&lt;code&gt;{input}&lt;/code&gt;" and "&lt;code&gt;{output}&lt;/code&gt;" are placeholders that refer to all of the
prerequisites and target of a rule, respectively.
In &lt;em&gt;Snakemake&lt;/em&gt;, placeholders are all wrapped in opening and closing brackets,
and are replaced with the value of that variable at runtime.
If you are familiar with modern Python format strings, that's where the syntax
comes from.&lt;/p&gt;
&lt;p&gt;Likewise&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule archive_results:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;isles.tsv&amp;quot;, &amp;quot;abyss.tsv&amp;quot;, &amp;quot;isles.png&amp;quot;, &amp;quot;abyss.png&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;zipf_results.tgz&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell:&lt;/span&gt;
&lt;span class="err"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;        rm -rf zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        mkdir zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        cp isles.tsv abyss.tsv isles.png abyss.png zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        tar -czf zipf_results.tgz zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        rm -r zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;can now be rewritten as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule archive_results:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;isles.tsv&amp;quot;, &amp;quot;abyss.tsv&amp;quot;, &amp;quot;isles.png&amp;quot;, &amp;quot;abyss.png&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;zipf_results.tgz&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell:&lt;/span&gt;
&lt;span class="err"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;        rm -rf zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        mkdir zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        cp {input} zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        tar -czf {output} zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        rm -r zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's a little less cluttered,
and still perfectly understandable once you know what the variables mean.
The best part, is that if I want to change the input files, I only need to
edit my snakefile in one place.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;```bash
snakemake clean
snakemake isles.tsv
``````````&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--Those extra backticks are because of Vim syntax highlighting.--&gt;

&lt;p&gt;You should get the same output as last time.
Internally, &lt;em&gt;Snakemake&lt;/em&gt; replaced "&lt;code&gt;{output}&lt;/code&gt;" with "&lt;code&gt;isles.tsv&lt;/code&gt;"
and "&lt;code&gt;{input}&lt;/code&gt;" with "&lt;code&gt;books/isles.txt&lt;/code&gt;"
before running the recipe.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Go ahead and rewrite all of the rules in Snakefile to minimize
repetition and take advantage of the "&lt;code&gt;{input}&lt;/code&gt;" and "&lt;code&gt;{output}&lt;/code&gt;"
placeholders.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Wildcard Filenames [10 minutes]&lt;/h3&gt;
&lt;p&gt;Another deviation from D.R.Y.:
We have nearly identical recipes for &lt;code&gt;abyss.tsv&lt;/code&gt; and &lt;code&gt;isles.tsv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It turns out we can replace &lt;em&gt;both&lt;/em&gt; of those rules with a single rule,
by telling &lt;em&gt;Snakemake&lt;/em&gt; about the relationships between filenames with
&lt;em&gt;wildcards&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Using wildcards looks like this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule wordcount:&lt;/span&gt;
&lt;span class="err"&gt;    input: &amp;quot;books/{name}.txt&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;{name}.tsv&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;scripts/wordcount.py {input} {output}&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we've replaced the book name with "&lt;code&gt;{name}&lt;/code&gt;".
The "&lt;code&gt;{name}&lt;/code&gt;" matches any part of the input filename between "&lt;code&gt;books/&lt;/code&gt;"
and "&lt;code&gt;.txt&lt;/code&gt;", and must be the same as "&lt;code&gt;{name}&lt;/code&gt;" in the output filename.
You don't have to use "name" as your wildcard name, and you should be
descriptive.&lt;/p&gt;
&lt;p&gt;This rule can be interpreted as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In order to build a file named &lt;code&gt;[something].tsv&lt;/code&gt; (the target)
find a file named &lt;code&gt;books/[that same something].txt&lt;/code&gt; (the prerequisite)
and run &lt;code&gt;scripts/wordcount.py [the prerequisite] [the target]&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice how helpful the automatic input/output variables were here.
This recipe will work no matter what stem is being matched!&lt;/p&gt;
&lt;p&gt;Go ahead and make this change in your snakefile.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;After you've replaced the two rules with one
rule using wildcards, try removing all of the products (&lt;code&gt;snakemake clean&lt;/code&gt;)
and rerunning the pipeline.&lt;/p&gt;
&lt;p&gt;Is anything different now that you're using the new, universal rule?&lt;/p&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Replace the rules for &lt;code&gt;abyss.png&lt;/code&gt; and &lt;code&gt;isles.png&lt;/code&gt;
with a single rule.&lt;/p&gt;
&lt;h4&gt;Challenge&lt;/h4&gt;
&lt;p&gt;Add &lt;code&gt;books/sierra.txt&lt;/code&gt; to your pipeline.&lt;/p&gt;
&lt;p&gt;(i.e. &lt;code&gt;snakemake all&lt;/code&gt; should plot the word counts and add the plots to
&lt;code&gt;zipf_results.tgz&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(If you'd prefer a pre-cooked snakefile: &lt;code&gt;cp .extra/Snakefile.2 Snakefile&lt;/code&gt;)&lt;/p&gt;
&lt;h2&gt;Scripts as prerequisites [10 minutes]&lt;/h2&gt;
&lt;p&gt;We've talked a lot about the power of &lt;em&gt;Snakemake&lt;/em&gt; for
rebuilding research outputs when input data changes.
When doing novel data analysis, however, it's very common for our &lt;em&gt;scripts&lt;/em&gt; to
be as or &lt;em&gt;more&lt;/em&gt; dynamic than the data.&lt;/p&gt;
&lt;p&gt;What happens when we edit our scripts instead of changing our data?&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;First, run &lt;code&gt;snakemake all&lt;/code&gt; so your analysis is up-to-date.&lt;/p&gt;
&lt;p&gt;Let's change the default number of entries in the rank/frequency
plot from 10 to 5.&lt;/p&gt;
&lt;p&gt;(Hint: edit the function definition for &lt;code&gt;plot_word_counts&lt;/code&gt; in
&lt;code&gt;plotcount.py&lt;/code&gt; to read &lt;code&gt;limit=5&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;Now run &lt;code&gt;snakemake all&lt;/code&gt; again.  What happened?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As it stands, we have to run &lt;code&gt;snakemake clean&lt;/code&gt; followed by &lt;code&gt;snakemake all&lt;/code&gt;
to update our analysis with the new script.
We're missing out on the benefits of incremental analysis when our scripts
are changing too.&lt;/p&gt;
&lt;p&gt;There must be a better way...and there is.
Scripts should be considered inputs too!&lt;/p&gt;
&lt;p&gt;Let's edit the rule for &lt;code&gt;{name}.png&lt;/code&gt; to include &lt;code&gt;plotcount.py&lt;/code&gt;
as an input.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rule plotcount:&lt;/span&gt;
&lt;span class="err"&gt;    input:&lt;/span&gt;
&lt;span class="err"&gt;        script=&amp;quot;scripts/plotcount.py&amp;quot;,&lt;/span&gt;
&lt;span class="err"&gt;        data=&amp;quot;{name}.tsv&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    output: &amp;quot;{name}.png&amp;quot;&lt;/span&gt;
&lt;span class="err"&gt;    shell: &amp;quot;{input.script} {input.data} {output}&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we've assigned names to our two inputs.&lt;/p&gt;
&lt;p&gt;This recipe works because "&lt;code&gt;{input.script}&lt;/code&gt;" is replaced with
"&lt;code&gt;scripts/plotcount.py&lt;/code&gt;"
and "&lt;code&gt;{input.data}&lt;/code&gt;" with the appropriate expansion of "&lt;code&gt;{name}.tsv&lt;/code&gt;".
When building &lt;code&gt;abyss.png&lt;/code&gt;, for instance,
"&lt;code&gt;{input.script} {input.data} {output}&lt;/code&gt;" becomes
"&lt;code&gt;scripts/plotcount.py abyss.tsv abyss.png&lt;/code&gt;", which is exactly what we want.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens when you run the pipeline after modifying your script again?&lt;/p&gt;
&lt;p&gt;(Changes to your script can be simulated with &lt;code&gt;touch plotcount.py&lt;/code&gt;.)&lt;/p&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Update your other rules to include the relevant scripts as inputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Final snakefile: &lt;code&gt;cp .extra/Snakefile.3 Snakefile&lt;/code&gt;)&lt;/p&gt;
&lt;h1&gt;Conclusion [1 minutes]&lt;/h1&gt;
&lt;p&gt;I hope that I've convinced you of the value of &lt;em&gt;Snakemake&lt;/em&gt; for data analysis.
What I have shown you today barely scratches the surface of the software's
functionality;
I encourage you to check out the &lt;a href="https://snakemake.readthedocs.io"&gt;website&lt;/a&gt;.
In my experience, though, the topics we've gone over today already provide
90% of the benefits:
we can forget about script names
and intermediate steps and focus instead on the output files that we want.
This &lt;a href="https://en.wikipedia.org/wiki/Declarative_programming"&gt;'declarative'&lt;/a&gt; approach to pipelines
pipelines has transformed the way I do data analysis.
I think it can do the same for you.&lt;/p&gt;</content><category term="Computing"></category><category term="teaching"></category><category term="programming"></category><category term="python"></category><category term="pipelines"></category><category term="bioinformatics"></category><category term="software"></category></entry><entry><title>Teaching Python by the (Note)Book</title><link href="//blog.byronjsmith.com/python-lesson-balance.html" rel="alternate"></link><published>2017-01-01T18:30:00-05:00</published><updated>2017-01-01T18:30:00-05:00</updated><author><name>Byron J. Smith</name></author><id>tag:blog.byronjsmith.com,2017-01-01:/python-lesson-balance.html</id><summary type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; I tried out a &lt;a href="https://gist.github.com/bsmith89/5eeb9e7da35bd6b8bf28ae884f6478ff"&gt;modified Python lesson&lt;/a&gt;
and I think it was successful at balancing learner motivation with teaching
foundational (and sometimes boring) concepts.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many ways, &lt;a href="https://software-carpentry.org"&gt;teaching Python to scientists&lt;/a&gt;
is easier than just about every other audience.
The learning objective is clear: write code to make my science more accurate,
more efficient, and more impactful.
The motivation is apparent: data is increasingly plentiful and increasingly
complex.
The learners are both engaged and prepared to put in the effort
required to develop new skills.&lt;/p&gt;
&lt;p&gt;But, despite all of the advantages, teaching &lt;em&gt;anybody&lt;/em&gt; to program is hard.&lt;/p&gt;
&lt;p&gt;In my experience, one of the most challenging trade-offs for lesson planners
is between motivating the material and teaching a mental model
for code execution.
For example, scientists are easily motivated by simple data munging and
plotting using &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt;;
these are features of the Python ecosystem that can convince â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;&lt;strong&gt;tl;dr:&lt;/strong&gt; I tried out a &lt;a href="https://gist.github.com/bsmith89/5eeb9e7da35bd6b8bf28ae884f6478ff"&gt;modified Python lesson&lt;/a&gt;
and I think it was successful at balancing learner motivation with teaching
foundational (and sometimes boring) concepts.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In many ways, &lt;a href="https://software-carpentry.org"&gt;teaching Python to scientists&lt;/a&gt;
is easier than just about every other audience.
The learning objective is clear: write code to make my science more accurate,
more efficient, and more impactful.
The motivation is apparent: data is increasingly plentiful and increasingly
complex.
The learners are both engaged and prepared to put in the effort
required to develop new skills.&lt;/p&gt;
&lt;p&gt;But, despite all of the advantages, teaching &lt;em&gt;anybody&lt;/em&gt; to program is hard.&lt;/p&gt;
&lt;p&gt;In my experience, one of the most challenging trade-offs for lesson planners
is between motivating the material and teaching a mental model
for code execution.
For example, scientists are easily motivated by simple data munging and
plotting using &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt;;
these are features of the Python ecosystem that can convince a graduate
student to pay attention to the material instead of answering emails.
Actually &lt;em&gt;using&lt;/em&gt; these features, however, requires a long list of basic
concepts: Python syntax, libraries, function calls, objects and methods,
conditionals, and variable assignment, to name just a few.&lt;/p&gt;
&lt;p&gt;A lesson planner can start from the basics, working in to these features
along the branches of the dependency tree, but that could require hours
(or even days) of "boring programming".
It's all too easy to dismiss learners who lose interest before you get
to the good stuff, but it is more a reflection of the materials and
instructor, than the students.&lt;/p&gt;
&lt;p&gt;At the other extreme, a lesson could start with working code,
or, in the Software Carpentry style the instructor could lead learners
through writing code that uses these features, before the concepts have
been fully introduced.
This in-at-the-deep-end approach quickly demonstrates exciting uses of Python,
but at the risk of intimidating learners, making them wonder if they're the
only one in the room who's confused by what's going on under the hood and what
all of the syntax means.
I'm not aware of any studies on this topic (if you are, please pass them my
way), but I'm willing to speculate that this second approach has
a higher risk of subjecting learners from underrepresented groups to stereotype
threat, a major risk when teaching a subject with a pervasive diversity
problem.&lt;/p&gt;
&lt;p&gt;Luckily for us all, there's a whole spectrum of approaches in between the
motivations-first and foundations-first extremes.
We can trust learners to self-motivate for a time, especially when we're
teaching scientists.
Attendees are there voluntarily (I would hope).
Likewise, learners will never have a perfect understanding of how their code is
working, regardless of how long you spend teaching the basics.
The key is to avoid unintentionally teaching pathological mental models that
are difficult for the instructor to diagnose and iterate beyond.&lt;/p&gt;
&lt;h1&gt;State of the Python lesson&lt;/h1&gt;
&lt;p&gt;As of this writing, the current default Python lesson for Software Carpentry is
&lt;a href="http://swcarpentry.github.io/python-novice-inflammation/"&gt;"Novice Inflammation"&lt;/a&gt;&lt;sup id="fnref:inflammation-commit"&gt;&lt;a class="footnote-ref" href="#fn:inflammation-commit"&gt;1&lt;/a&gt;&lt;/sup&gt;.
I have &lt;a href="//blog.byronjsmith.com/swc-python-lesson.html"&gt;written previously&lt;/a&gt; about my experience
with the lesson, and have not been shy with my criticism.
There is a &lt;em&gt;lot&lt;/em&gt; to be positive about in the composition of Inflammation.
It has been an effective approach to teaching Python to what at this point
must be several thousand workshop attendees.&lt;/p&gt;
&lt;p&gt;However, this post is about how we can do better.
My primary criticism focuses on the first section:
&lt;a href="http://swcarpentry.github.io/python-novice-inflammation/01-numpy/"&gt;"Analyzing Patient Data"&lt;/a&gt;.
The approach here falls towards the motivation-first extreme.
Learners are shown how one might go from raw data in a CSV to heatmaps
and line plots, two useful skills.&lt;/p&gt;
&lt;p&gt;The downside, however, is that this happens without fully explaining the
syntax, what libraries are, &lt;code&gt;numpy&lt;/code&gt; arrays versus Python lists, &lt;code&gt;dtypes&lt;/code&gt; vs
built-in Python types, and more.
I think that by using this particular motivating example while glossing over
those details we're giving learners a challenging mental model to iterate
beyond.
What's more, I believe that this results in a &lt;em&gt;diversity&lt;/em&gt; of models
making later instruction more likely to leave some learners behind.
Novice Inflammation also gets stuck in the weeds over difficult concepts which,
in my opinion, aren't nearly as important for learners, for example,
accumulating over particular axes in &lt;code&gt;numpy&lt;/code&gt; arrays.&lt;/p&gt;
&lt;p&gt;For this and other reasons Greg Wilson spearheaded an attempt to
reinvent the Python lesson.
The &lt;a href="http://swcarpentry.github.io/python-novice-gapminder/"&gt;"Novice Gapminder" lesson&lt;/a&gt;&lt;sup id="fnref:gapminder-commit"&gt;&lt;a class="footnote-ref" href="#fn:gapminder-commit"&gt;2&lt;/a&gt;&lt;/sup&gt;
is a from-scratch re-write.
It's worth noting that SWC's normal pull-request model for lesson development
is unable to accommodate a major overhaul like this one.&lt;/p&gt;
&lt;p&gt;Gapminder is different in several ways, for instance using &lt;code&gt;pandas&lt;/code&gt; as a focal
library instead of &lt;code&gt;numpy&lt;/code&gt;.
Notably for this commentary, though, it also takes a much more gradual approach
to motivating the material.
&lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; are not introduced until the end of the first
half-day,
and only &lt;em&gt;after&lt;/em&gt; a thorough discussion of variable assignment, functions, and
data types.
The Gapminder lesson also appears to lack the distractions and rabbit holes
that I've criticized in Inflammation.&lt;/p&gt;
&lt;p&gt;Overall, I think Gapminder hits a superior balance between motivation
and basics, while also improve the structure and refining the details.
I have to applaud everyone who's contributed to its development.
I've now taught from the new lesson once, and co-instructed a workshop that
used the first half.
The improvements in the design were apparent both times.
I expect it to be well received by the SWC community when it becomes the
default.&lt;/p&gt;
&lt;h1&gt;Continual improvement&lt;/h1&gt;
&lt;p&gt;That's not to say, however, that it cannot be improved.
The same motivation-vs-foundations question has already come up in
&lt;a href="https://github.com/swcarpentry/python-novice-gapminder/issues/113"&gt;a discussion on GitHub&lt;/a&gt;.
A proposal was made to delay the use of &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; until the
second half, further front-loading the basics.
My personal opinion, having taught the lesson is that
this is unnecessary.
With the Gapminder lesson, by the time we got to these more advanced topics at
the end of the first half-day, learners appeared to be ready for the material,
comfortably updating their mental models in an appropriate way.
And, thankfully, I also didn't notice a loss of engagement due to the delayed
pay-off.&lt;/p&gt;
&lt;p&gt;Like many trade-offs in lesson design, the optimal position on the
motivations/foundations spectrum is context dependent.
I would focus on cool application instead of basic concepts
for the first session if I were
teaching high school students or any learners skeptical about the utility of
the material.
A room full of scientists who were there specifically to learn Python, however,
could probably tolerate even more front-loading of syntax and control-flow.
A framework to help instructors customize the materials for their audience
would be a very useful addition.&lt;/p&gt;
&lt;p&gt;The main purpose of this post is to nominate a slightly different approach
which introduces an advanced example early in the lesson without the
risk (I believe) of intimidating learners.&lt;/p&gt;
&lt;p&gt;In December 2016 I co-instructed a (not officially SWC)
&lt;a href="https://umswc.github.io/2016-12-14-umich/"&gt;workshop&lt;/a&gt; which taught Python over two half-day sessions to
about 20 learners, primarily graduate students in the biological sciences.&lt;/p&gt;
&lt;p&gt;My co-instructor, Jackie Cohen (&lt;a href="https://twitter.com/jczetta"&gt;\@jczetta&lt;/a&gt;),
taught the first half-day using the Gapminder lesson.
The positive reception from learners to the first half of the material was
testament not only to her skillful instruction, but also the quality of the
design.&lt;/p&gt;
&lt;p&gt;I then taught the second day with the
same gapminder dataset and covering the same
topics as the normal materials, but using &lt;a href="https://gist.github.com/bsmith89/5eeb9e7da35bd6b8bf28ae884f6478ff"&gt;a custom lesson plan&lt;/a&gt;.
Inspired by &lt;a href="https://github.com/swcarpentry/python-novice-gapminder/issues/113#issuecomment-256230540"&gt;a comment&lt;/a&gt; on the Gapminder GitHub repository,
I constructed a "realistic" analysis of the gapminder data
as a Jupyter notebook.
In particular, the notebook includes code to generate a fairly involved
figure telling a story about the relationship between per-capita GDP and
life-expectancy.&lt;/p&gt;
&lt;p&gt;&lt;img alt="An example visualization of the gapminder data." src="//blog.byronjsmith.com/static/images/gapminder-analysis.png"&gt;&lt;/p&gt;
&lt;p&gt;I started the second day by having learners download and run this notebook,
demonstrating the quality of analyses they could produce with fewer than 100
lines of code.
By &lt;em&gt;not&lt;/em&gt; live-coding, and &lt;em&gt;not&lt;/em&gt; expecting the learners to type along during the
introduction, I believe this approach minimizes the likelihood of intimidating
the learners with syntax.
To that end, I also did not walk through the code itself, but instead focused
on describing the overarching flow of the analysis:
loading external data, selecting a subset, plotting two columns as a
scatter-plot with a third column determining the size of the points, running a
linear regression, and plotting a best-fit line.
The purpose of this introduction was purely to motivate the material, not
to introduce the concepts.&lt;/p&gt;
&lt;p&gt;I then had them open a new, empty notebook, and the remainder of the lesson
(which &lt;em&gt;was&lt;/em&gt; done in the traditional live-coding style)
then revolved around reconstructing the same analysis from scratch,
a thematic unification, that I found to be elegant.
Since the pre-constructed analysis made use of for-loops, if-statements,
and functions, I was able to limit my use of foo/bar style examples and
quickly return to the core analysis demonstrating the use of these elements
in a realistic setting.
Our workshop was advertised as an introduction to Jupyter notebooks, data
manipulation, and plotting, (as well as novice Python) so a significant
fraction of the time was spent on these topics and libraries instead of more
foundational concepts.&lt;/p&gt;
&lt;h1&gt;Where to go from here?&lt;/h1&gt;
&lt;p&gt;I found this approach to be quite successful.
In-person and exit survey feedback has been uniformly positive and
learners appeared to have achieved most or all of the learning objectives
of the core Gapminder lesson.
While the "realistic analysis" approach sounds more like the
&lt;a href="http://www.datacarpentry.org/"&gt;Data Carpentry&lt;/a&gt;
style, in this particular case it was a great fit for the Software Carpentry
learning objectives.&lt;/p&gt;
&lt;p&gt;I believe that this model could be implemented in the core Gapminder lesson,
perhaps starting in the second half (as we did), or with a different
example notebook for each half-day.
That would, however, entail modifying most or all of the sections to
focus on the new unified example.
Is it worth expending the tens of hours required to implement it?
Even if it were implemented, I'm not convinced that the SWC lesson development
model makes this kind of large-scale refactoring feasible.&lt;/p&gt;
&lt;p&gt;As an alternative to submitting a pull request,
I'm hoping that I can convince a few instructors to try it out for themselves.
Positive experiences with an unofficial fork makes patching the main branch a
more rational investment.
I've already been &lt;a href="http://lists.software-carpentry.org/pipermail/discuss/2016-May/004529.html"&gt;evangelizing&lt;/a&gt; in a similar way for an
&lt;a href="https://github.com/bsmith89/git-novice-outline"&gt;alternative Git lesson&lt;/a&gt;,
sharing my immature outline and encouraging folks to try it out
themselves.
Is there a better approach to making medium to large changes to the design of
a SWC lesson?&lt;/p&gt;
&lt;p&gt;In conclusion: I think we need additional discussion (and data) about the
motivations/foundations trade-off in our lessons.
I'd also like to hear your thoughts on the best way to lobby for and introduce
moderately sized changes to the core materials.
What do you think about my approach?
If you're feeling brave, please try it out and let me know how it goes!&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:inflammation-commit"&gt;
&lt;p&gt;HEAD at
&lt;a href="https://github.com/swcarpentry/python-novice-inflammation/tree/030f3fbd3006cea06e42bbd14a62ddb33098b9f6"&gt;030f3fbd30&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:inflammation-commit" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:gapminder-commit"&gt;
&lt;p&gt;HEAD at &lt;a href="https://github.com/swcarpentry/python-novice-gapminder/tree/e303e6a9d309bdcbcfb370c8125b7792d4096968"&gt;e303e6a9d3&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:gapminder-commit" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Education"></category><category term="software-carpentry"></category><category term="teaching"></category><category term="programming"></category><category term="python"></category><category term="jupyter"></category></entry><entry><title>Tutorial: Reproducible bioinformatics pipelines using GNU Make</title><link href="//blog.byronjsmith.com/make-analysis.html" rel="alternate"></link><published>2016-03-04T12:00:00-05:00</published><updated>2017-11-21T09:30:00-05:00</updated><author><name>Byron J. Smith</name></author><id>tag:blog.byronjsmith.com,2016-03-04:/make-analysis.html</id><summary type="html">&lt;p&gt;&lt;em&gt;WARNING: Because of the Markdown rendering of this blog, tab characters
have been replaced with 4 spaces in code blocks.
For this reason, &lt;strong&gt;the makefile code will not work&lt;/strong&gt; when copied directly from
the post.
Instead, you must first replace all 4-space indents with a tab character.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For most projects with moderate to intense data analysis you should
consider using &lt;em&gt;Make&lt;/em&gt;.
Some day I'll write a post telling you why, but for now check out
&lt;a href="http://zmjones.com/make/"&gt;this post&lt;/a&gt; by Zachary M. Jones&lt;sup id="fnref:similar-title"&gt;&lt;a class="footnote-ref" href="#fn:similar-title"&gt;1&lt;/a&gt;&lt;/sup&gt;.
If you're already convinced, or just want to see what it's all about, read on.&lt;/p&gt;
&lt;p&gt;This post is the clone of a tutorial that I wrote for Titus Brown's
week-long &lt;a href="https://dib-training.readthedocs.org/en/pub/2016-02-08-bodega.html"&gt;Bioinformatics Workshop&lt;/a&gt; at UC Davis's Bodega
Marine Laboratory in February, 2016.
For now, the live tutorial lives in &lt;a href="https://github.com/bsmith89/make-bml"&gt;a Github repository&lt;/a&gt;,
although I eventually want to merge all of the good parts into the Software
Carpentry &lt;a href="https://swcarpentry.github.io/make-novice"&gt;Make lesson â€¦&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;WARNING: Because of the Markdown rendering of this blog, tab characters
have been replaced with 4 spaces in code blocks.
For this reason, &lt;strong&gt;the makefile code will not work&lt;/strong&gt; when copied directly from
the post.
Instead, you must first replace all 4-space indents with a tab character.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For most projects with moderate to intense data analysis you should
consider using &lt;em&gt;Make&lt;/em&gt;.
Some day I'll write a post telling you why, but for now check out
&lt;a href="http://zmjones.com/make/"&gt;this post&lt;/a&gt; by Zachary M. Jones&lt;sup id="fnref:similar-title"&gt;&lt;a class="footnote-ref" href="#fn:similar-title"&gt;1&lt;/a&gt;&lt;/sup&gt;.
If you're already convinced, or just want to see what it's all about, read on.&lt;/p&gt;
&lt;p&gt;This post is the clone of a tutorial that I wrote for Titus Brown's
week-long &lt;a href="https://dib-training.readthedocs.org/en/pub/2016-02-08-bodega.html"&gt;Bioinformatics Workshop&lt;/a&gt; at UC Davis's Bodega
Marine Laboratory in February, 2016.
For now, the live tutorial lives in &lt;a href="https://github.com/bsmith89/make-bml"&gt;a Github repository&lt;/a&gt;,
although I eventually want to merge all of the good parts into the Software
Carpentry &lt;a href="https://swcarpentry.github.io/make-novice"&gt;Make lesson&lt;/a&gt; (&lt;a href="https://github.com/swcarpentry/make-novice"&gt;repository&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I'm posting this tutorial because I think it's a good introduction to the
analysis pipeline approach I have been slowly adopting over the last several
years.
This approach is even more deeply enshrined in a
&lt;a href="https://github.com/bsmith89/compbio-template"&gt;project template&lt;/a&gt; that I have been developing.
You can think of this tutorial as a gentle introduction to the motivation
for that template.&lt;/p&gt;
&lt;p&gt;The goals of this tutorial are three-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Teach GNU Make basics,&lt;/li&gt;
&lt;li&gt;Demonstrate the use of general best-practices (version control, README's, etc.), and&lt;/li&gt;
&lt;li&gt;Describe my preferred way to organize projects.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While much of the material is original, it was heavily inspired by the
&lt;a href="https://swcarpentry.github.io/make-novice"&gt;Software Carpentry Make lesson&lt;/a&gt;
which is licensed &lt;a href="https://creativecommons.org/licenses/by/4.0/legalcode"&gt;CC-BY 4.0.&lt;/a&gt;,
and the example project is almost identical.&lt;/p&gt;
&lt;p&gt;And it is divided into five sections:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#setup"&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#motivation"&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#makefile-basics"&gt;Makefile basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#make-features"&gt;&lt;em&gt;Make&lt;/em&gt; features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#best-practices-for-make-based-projects"&gt;Best practices for &lt;em&gt;Make&lt;/em&gt;-based projects&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;p&gt;(Estimated time: 15 Minutes)&lt;/p&gt;
&lt;p&gt;This tutorial was designed for UNIX systems and has been tested
on Amazon EC2 using the
Ubuntu Server 14.04 LTS image and a "m3.medium" instance.
If you would like to use Windows, Git-Bash (packaged with Git for Windows)
is probably your best bet, although it has not been tested on that platform.&lt;/p&gt;
&lt;p&gt;For this lesson we will be using an already prepared set of files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl https://codeload.github.com/bsmith89/make-example/tar.gz/v1.0-snap &lt;span class="se"&gt;\&lt;/span&gt;
    &amp;gt; make-example-1.0-snap.tgz
tar -xzf make-example-1.0-snap.tgz
&lt;span class="nb"&gt;cd&lt;/span&gt; make-example-1.0-snap
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's take a look at the files we will be working with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install tree
tree
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;tree&lt;/code&gt; command produces a handy tree-diagram of the directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;   &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;   &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;   &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="k"&gt;last&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;   &lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;LICENSE_TEXTS&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;
&lt;span class="err"&gt;â”‚&lt;/span&gt;   &lt;span class="err"&gt;â””â”€â”€&lt;/span&gt; &lt;span class="n"&gt;sierra&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;txt&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;LICENSE&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;matplotlibrc&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;plotcount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="err"&gt;â”œâ”€â”€&lt;/span&gt; &lt;span class="n"&gt;README&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md&lt;/span&gt;
&lt;span class="err"&gt;â””â”€â”€&lt;/span&gt; &lt;span class="n"&gt;wordcount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;

&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="n"&gt;files&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Be sure that you also have &lt;em&gt;Python 3&lt;/em&gt;, &lt;em&gt;Git&lt;/em&gt;, and &lt;em&gt;GNU Make&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get install python3 git make
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Configure git.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git config --global user.name &lt;span class="s2"&gt;&amp;quot;Your Name&amp;quot;&lt;/span&gt;
git config --global user.email you@example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Install matplotlib.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get install python3-matplotlib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;(Estimated time: 30 minutes)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The most frequently-occurring word occurs approximately twice as
often as the second most frequent word. This is
&lt;a href="http://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Zipf's Law&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's imagine that instead of computational biology we're interested in
testing Zipf's law in some of our favorite books.
We've compiled our raw data, the books we want to analyze
(check out &lt;code&gt;head books/isles.txt&lt;/code&gt;)
and have prepared several Python scripts that together make up our
analysis pipeline.&lt;/p&gt;
&lt;p&gt;Before we begin, add a README to your project describing what we intend
to do.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;nano README.md
&lt;span class="c1"&gt;# Describe what you&amp;#39;re going to do. (e.g. &amp;quot;Test Zipf&amp;#39;s Law&amp;quot;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first step is to count the frequency of each word in the book.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./wordcount.py books/isles.txt isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(The leading '&lt;code&gt;./&lt;/code&gt;' is required so that Bash knows we're executing
a file in the current directory rather than a command in our path.)&lt;/p&gt;
&lt;p&gt;Let's take a quick peek at the result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head -5 isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;shows us the top 5 lines in the output file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;the 3822    6.7371760973&lt;/span&gt;
&lt;span class="err"&gt;of  2460    4.33632998414&lt;/span&gt;
&lt;span class="err"&gt;and 1723    3.03719372466&lt;/span&gt;
&lt;span class="err"&gt;to  1479    2.60708619778&lt;/span&gt;
&lt;span class="err"&gt;a   1308    2.30565838181&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each row shows the word itself, the number of occurrences of that
word, and the number of occurrences as a percentage of the total
number of words in the text file.&lt;/p&gt;
&lt;p&gt;We can do the same thing for a different book:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./wordcount.py books/abyss.txt abyss.dat
head -5 abyss.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, let's visualize the results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./plotcount.py isles.dat ascii
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;ascii&lt;/code&gt; argument has been added so that we get a text-based
bar-plot printed to the screen.&lt;/p&gt;
&lt;p&gt;The script is also able to render a graphical bar-plot using matplotlib
and save the figure to a given file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./plotcount.py isles.dat isles.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Together these scripts implement a common workflow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read a data file.&lt;/li&gt;
&lt;li&gt;Perform an analysis on this data file.&lt;/li&gt;
&lt;li&gt;Write the analysis results to a new file.&lt;/li&gt;
&lt;li&gt;Plot a graph of the analysis results.&lt;/li&gt;
&lt;li&gt;Save the graph as an image, so we can put it in a paper.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Writing a "master" script&lt;/h2&gt;
&lt;p&gt;Running this pipeline for one book is pretty easy using the command-line.
But once the number of files and the number of steps in the pipeline
expands, this can turn into a lot of work.
Plus, no one wants to sit and wait for a command to finish, even just for 30
seconds.&lt;/p&gt;
&lt;p&gt;The most common solution to the tedium of data processing is to write
a master script that runs the whole pipeline from start to finish.&lt;/p&gt;
&lt;p&gt;We can make a new file, &lt;code&gt;run_pipeline.sh&lt;/code&gt; that contains:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class="c1"&gt;# USAGE: bash run_pipeline.sh&lt;/span&gt;
&lt;span class="c1"&gt;# to produce plots for isles and abyss.&lt;/span&gt;

./wordcount.py isles.txt isles.dat
./wordcount.py abyss.txt abyss.dat

./plotcount.py isles.dat isles.png
./plotcount.py abyss.dat abyss.png

&lt;span class="c1"&gt;# Now archive the results in a tarball so we can share them with a colleague.&lt;/span&gt;
rm -rf zipf_results
mkdir zipf_results
mv isles.dat abyss.dat isles.png abyss.png zipf_results/
tar -czf zipf_results.tgz zipf_results
rm -r zipf_results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This master script solved several problems in computational reproducibility:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It explicitly documents our pipeline,
    making communication with colleagues (and our future selves) more efficient.&lt;/li&gt;
&lt;li&gt;It allows us to type a single command, &lt;code&gt;bash run_pipeline.sh&lt;/code&gt;, to
    reproduce the full analysis.&lt;/li&gt;
&lt;li&gt;It prevents us from &lt;em&gt;repeating&lt;/em&gt; typos or mistakes.
    You might not get it right the first time, but once you fix something
    it'll (probably) stay that way.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To continue with the Good Ideas, let's put everything under version control.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git init
git add README.md
git commit -m &lt;span class="s2"&gt;&amp;quot;Starting a new project.&amp;quot;&lt;/span&gt;
git add wordcount.py plotcount.py matplotlibrc
git commit -m &lt;span class="s2"&gt;&amp;quot;Write scripts to test Zipf&amp;#39;s law.&amp;quot;&lt;/span&gt;
git add run_pipeline.sh
git commit -m &lt;span class="s2"&gt;&amp;quot;Write a master script to run the pipeline.&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice that we didn't version control any of the products of our analysis.
We'll talk more about this at the end of the tutorial.&lt;/p&gt;
&lt;p&gt;A master script is a good start, but it has a few shortcomings.&lt;/p&gt;
&lt;p&gt;Let's imagine that we adjusted the width of the bars in our plot
produced by &lt;code&gt;plotcount.py&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;nano plotcount.py
&lt;span class="c1"&gt;# In the definition of plot_word_counts replace:&lt;/span&gt;
&lt;span class="c1"&gt;#    width = 1.0&lt;/span&gt;
&lt;span class="c1"&gt;# with:&lt;/span&gt;
&lt;span class="c1"&gt;#    width = 0.8&lt;/span&gt;
git add plotcount.py
git commit -m &lt;span class="s2"&gt;&amp;quot;Fix the bar width.&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we want to recreate our figures.
We &lt;em&gt;could&lt;/em&gt; just &lt;code&gt;bash run_pipeline.sh&lt;/code&gt; again.
That would work, but it could also be a big pain if counting words takes
more than a few seconds.
The word counting routine hasn't changed; we shouldn't need to recreate
those files.&lt;/p&gt;
&lt;p&gt;Alternatively, we could manually rerun the plotting for each word-count file
and recreate the tarball.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; file in *.dat&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    ./plotcount.py &lt;span class="nv"&gt;$file&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;file&lt;/span&gt;&lt;span class="p"&gt;/.dat/.png&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;

rm -rf zipf_results
mkdir zipf_results
mv isles.dat abyss.dat isles.png abyss.png zipf_results/
tar -czf zipf_results.tgz zipf_results
rm -r zipf_results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;But then we don't get many of the benefits of having a master script in
the first place.&lt;/p&gt;
&lt;p&gt;Another popular option is to comment out a subset of the lines in
&lt;code&gt;run_pipeline.sh&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="ch"&gt;#!/usr/bin/env bash&lt;/span&gt;
&lt;span class="c1"&gt;# USAGE: bash run_pipeline.sh&lt;/span&gt;
&lt;span class="c1"&gt;# to produce plots for isles and abyss.&lt;/span&gt;

&lt;span class="c1"&gt;# These lines are commented out because they don&amp;#39;t need to be rerun.&lt;/span&gt;
&lt;span class="c1"&gt;#./wordcount.py isles.txt isles.dat&lt;/span&gt;
&lt;span class="c1"&gt;#./wordcount.py abyss.txt abyss.dat&lt;/span&gt;

./plotcount.py isles.dat isles.png
./plotcount.py abyss.dat abyss.png

&lt;span class="c1"&gt;# Now archive the results in a tarball so we can share them with a colleague.&lt;/span&gt;
rm -rf zipf_results
mkdir zipf_results
mv isles.dat abyss.dat isles.png abyss.png zipf_results/
tar -czf zipf_results.tgz zipf_results
rm -r zipf_results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Followed by &lt;code&gt;bash run_pipeline.sh&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But this process, and subsequently undoing it,
can be a hassle and source of errors in complicated pipelines.&lt;/p&gt;
&lt;p&gt;What we really want is an executable &lt;em&gt;description&lt;/em&gt; of our pipeline that
allows software to do the tricky part for us:
figuring out what steps need to be rerun.
It would also be nice if this tool encourage a &lt;em&gt;modular&lt;/em&gt; analysis
and reusing instead of rewriting parts of our pipeline.
As an added benefit, we'd like it all to play nice with the other
mainstays of reproducible research: version control, Unix-style tools,
and a variety of scripting languages.&lt;/p&gt;
&lt;h1&gt;Makefile basics&lt;/h1&gt;
&lt;p&gt;(Estimated time: 45 minutes)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Make&lt;/em&gt; is a computer program originally designed to automate the compilation
and installation of software.
&lt;em&gt;Make&lt;/em&gt; automates the process of building target files through a series of
discrete steps.
Despite it's original purpose, this design makes it a great fit for
bioinformatics pipelines, which often work by transforming data from one form
to another (e.g. &lt;em&gt;raw data&lt;/em&gt; &amp;#8594; &lt;em&gt;word counts&lt;/em&gt; &amp;#8594; &lt;em&gt;???&lt;/em&gt; &amp;#8594; &lt;em&gt;profit&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;For this tutorial we will be using an implementation of &lt;em&gt;Make&lt;/em&gt; called
&lt;em&gt;GNU Make&lt;/em&gt;, although others exist.&lt;/p&gt;
&lt;h2&gt;A simple Makefile&lt;/h2&gt;
&lt;p&gt;Let's get started writing a description of our analysis for &lt;em&gt;Make&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Open up a file called &lt;code&gt;Makefile&lt;/code&gt; in your editor of choice (e.g. &lt;code&gt;nano Makefile&lt;/code&gt;)
and add the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;isles.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/&lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./wordcount.py books/isles.txt isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We have now written the simplest, non-trivial Makefile&lt;sup id="fnref:makefile-name"&gt;&lt;a class="footnote-ref" href="#fn:makefile-name"&gt;2&lt;/a&gt;&lt;/sup&gt;.
It is pretty reminiscent of one of the lines from our master script.
It is a good bet that you can figure out what this Makefile does.&lt;/p&gt;
&lt;p&gt;Be sure to notice a few syntactical items.&lt;/p&gt;
&lt;p&gt;The part before the colon is called the &lt;strong&gt;target&lt;/strong&gt; and the part after is our
list of &lt;strong&gt;prerequisites&lt;/strong&gt; (there is just one in this case).
This first line is followed by an indented section called the &lt;strong&gt;recipe&lt;/strong&gt;.
The whole thing is together called a &lt;strong&gt;rule&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Notice that the indent is &lt;em&gt;not&lt;/em&gt; multiple spaces, but is instead a single tab
character.
This is the first gotcha in makefiles.
If the difference between spaces and a tab character isn't obvious in your
editor of choice, try moving your cursor from one side of the tab to the other.
It should &lt;em&gt;jump&lt;/em&gt; four or more spaces.
If your recipe is not indented with a tab character it is likely to not work.&lt;/p&gt;
&lt;p&gt;Notice that this recipe is exactly the same as the analogous step in our
master shell script.
This is no coincidence; &lt;em&gt;Make&lt;/em&gt; recipes &lt;em&gt;are&lt;/em&gt; shell scripts.
The first line (&lt;em&gt;target&lt;/em&gt;: &lt;em&gt;prerequisites&lt;/em&gt;) explicitly declares two details
that were implicit in our pipeline script:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We are generating a file called &lt;code&gt;isles.dat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Creating this file requires &lt;code&gt;books/isles.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We'll think about our pipeline as a network of files that are dependent
on one another.
Right now our Makefile describes a pretty simple &lt;strong&gt;dependency graph&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;books/isles.txt&lt;/code&gt; &amp;#8594; &lt;code&gt;isles.dat&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where the "&amp;#8594;" is pointing from requirements to targets.&lt;/p&gt;
&lt;p&gt;Don't forget to commit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add Makefile
git commit -m &lt;span class="s2"&gt;&amp;quot;Start converting master script into a Makefile.&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Running &lt;em&gt;Make&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Now that we have a (currently incomplete) description of our pipeline,
let's use &lt;em&gt;Make&lt;/em&gt; to execute it.&lt;/p&gt;
&lt;p&gt;First, remove the previously generated files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;rm *.dat *.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;make isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should see the following print to the terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;./wordcount.py books/isles.txt isles.dat&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By default, &lt;em&gt;Make&lt;/em&gt; prints the recipes that it executes&lt;sup id="fnref:makefile-identification"&gt;&lt;a class="footnote-ref" href="#fn:makefile-identification"&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Let's see if we got what we expected.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;head -5 isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first 5 lines of that file should look exactly like before.&lt;/p&gt;
&lt;h2&gt;Rerunning &lt;em&gt;Make&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Let's try running &lt;em&gt;Make&lt;/em&gt; the same way again.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;make isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This time, instead of executing the same recipe,
&lt;em&gt;Make&lt;/em&gt; prints &lt;code&gt;make: Nothing to be done for 'isles.dat'.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;What's happening here?&lt;/p&gt;
&lt;p&gt;When you ask &lt;em&gt;Make&lt;/em&gt; to make &lt;code&gt;isles.dat&lt;/code&gt; it first looks at
the modification time of that target.
Next it looks at the modification time for the target's prerequisites.
If the target is newer than the prerequisites &lt;em&gt;Make&lt;/em&gt; decides that
the target is up-to-date and does not need to be remade.&lt;/p&gt;
&lt;p&gt;Much has been said about using modification times as the cue for remaking
files.
This can be another &lt;em&gt;Make&lt;/em&gt; gotcha, so keep it in mind.&lt;/p&gt;
&lt;p&gt;If you want to induce the original behavior, you just have to
change the modification time of &lt;code&gt;books/isles.txt&lt;/code&gt; so that it is newer
than &lt;code&gt;isles.dat&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;touch books/isles.txt
make isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The original behavior is restored.&lt;/p&gt;
&lt;p&gt;Sometimes you just want &lt;em&gt;Make&lt;/em&gt; to tell you what it thinks about the current
state of your files.
&lt;code&gt;make --dry-run isles.dat&lt;/code&gt; will print &lt;em&gt;Make&lt;/em&gt;'s execution plan, without
actually carrying it out.
The flag can be abbreviated as &lt;code&gt;-n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you don't pass a target as an argument to make (i.e. just run &lt;code&gt;make&lt;/code&gt;)
it will assume that you want to build the first target in the Makefile.&lt;/p&gt;
&lt;h2&gt;More recipes&lt;/h2&gt;
&lt;p&gt;Now that &lt;em&gt;Make&lt;/em&gt; knows how to build &lt;code&gt;isles.dat&lt;/code&gt;,
we can add a rule for plotting those results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;isles.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt;
    ./plotcount.py isles.dat isles.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The dependency graph now looks like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;books/isles.txt&lt;/code&gt; &amp;#8594; &lt;code&gt;isles.dat&lt;/code&gt; &amp;#8594; &lt;code&gt;isles.png&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let's add a few more recipes to our Makefile.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;abyss.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/&lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./wordcount.py books/abyss.txt abyss.dat

&lt;span class="nf"&gt;zipf_results.tgz&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt;
    rm -rf zipf_results/
    mkdir zipf_results/
    cp isles.dat abyss.dat isles.png abyss.png zipf_results/
    tar -czf zipf_results.tgz zipf_results/
    rm -r zipf_results/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And commit the changes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add Makefile
git commit -m &lt;span class="s2"&gt;&amp;quot;Add recipes for abyss counts, isles plotting, and the final archive.&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here the recipe for &lt;code&gt;zipf_results.tgz&lt;/code&gt; involves running a series of
shell commands.
When building the archive, &lt;em&gt;Make&lt;/em&gt; will run each line successively unless
any return an error.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Question&lt;/h4&gt;
&lt;p&gt;Without doing it, what happens if you run &lt;code&gt;make isles.png&lt;/code&gt;?&lt;/p&gt;
&lt;h4&gt;Challenge&lt;/h4&gt;
&lt;p&gt;What does the dependency graph look like for your Makefile?&lt;/p&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens if you run &lt;code&gt;make zipf_results.tgz&lt;/code&gt; right now?&lt;/p&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Write a recipe for &lt;code&gt;abyss.png&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once you've written a recipe for &lt;code&gt;abyss.png&lt;/code&gt; you should be able to
run &lt;code&gt;make zipf_results.tgz&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's delete all of our files and try it out.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;rm abyss.* isles.*
make zipf_results.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You should get the something like the following output
(the order may be different)
to your terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;./wordcount.py books/abyss.txt abyss.dat&lt;/span&gt;
&lt;span class="err"&gt;./wordcount.py books/isles.txt isles.dat&lt;/span&gt;
&lt;span class="err"&gt;./plotcount.py abyss.dat abyss.png&lt;/span&gt;
&lt;span class="err"&gt;./plotcount.py isles.dat isles.png&lt;/span&gt;
&lt;span class="err"&gt;rm -rf zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;mkdir zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;cp isles.dat abyss.dat isles.png abyss.png zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;tar -czf zipf_results.tgz zipf_results/&lt;/span&gt;
&lt;span class="err"&gt;rm -r zipf_results/&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since you asked for &lt;code&gt;zipf_results.tgz&lt;/code&gt; &lt;em&gt;Make&lt;/em&gt; looked first for that file.
Not finding it, &lt;em&gt;Make&lt;/em&gt; looked for its prerequisites.
Since none of those existed it remade the ones it could,
&lt;code&gt;abyss.dat&lt;/code&gt; and &lt;code&gt;isles.dat&lt;/code&gt;.
Once those were finished it was able to make &lt;code&gt;abyss.png&lt;/code&gt; and
&lt;code&gt;isles.png&lt;/code&gt;, before finally building &lt;code&gt;zipf_results.tgz&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You may also have gotten an additional line in your output similar to the
following.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;rm abyss.dat isles.dat abyss.png isles.png&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Because you only asked for &lt;code&gt;zipf_results.tgz&lt;/code&gt;, &lt;em&gt;Make&lt;/em&gt; thinks its doing
you a favor by deleting the intermediate files.
As computational biologists we know to never trust our analyses until they've
been tested and intermediate files are a valuable audit trail.
To prevent the default behavior, add the following to your Makefile.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;.SECONDARY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now remove the outputs and rerun your pipeline.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;rm zipf_results.tgz *.dat *.png
make zipf_results.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;.SECONDARY&lt;/code&gt; is one of a handful of &lt;strong&gt;special targets&lt;/strong&gt; used to control &lt;em&gt;Make&lt;/em&gt;'s
behavior.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens if you &lt;code&gt;touch abyss.dat&lt;/code&gt; and
then &lt;code&gt;make zipf_results.tgz&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add Makefile
git commit -m &lt;span class="s2"&gt;&amp;quot;Finish translating pipeline script to a Makefile.&amp;quot;&lt;/span&gt;
git status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice all the files that &lt;em&gt;Git&lt;/em&gt; wants to be tracking?
Like before, we're not going to version control any of the intermediate
or final products of our pipeline.
To reflect this fact add a &lt;code&gt;.gitignore&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;*.dat&lt;/span&gt;
&lt;span class="err"&gt;*.png&lt;/span&gt;
&lt;span class="err"&gt;zipf_results.tgz&lt;/span&gt;
&lt;span class="err"&gt;LICENSE.md&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add .gitignore
git commit -m &lt;span class="s2"&gt;&amp;quot;Have git ignore intermediate data files.&amp;quot;&lt;/span&gt;
git status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Phony targets&lt;/h2&gt;
&lt;p&gt;Sometimes its nice to have targets that don't refer to actual files.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;all&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;zipf_results&lt;/span&gt;.&lt;span class="n"&gt;tgz&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Even though this rule doesn't have a recipe, it does have prerequisites.
Now, when you run &lt;code&gt;make all&lt;/code&gt; &lt;em&gt;Make&lt;/em&gt; will do what it needs to to bring
all three of those targets up to date.&lt;/p&gt;
&lt;p&gt;It is traditional for "&lt;code&gt;all:&lt;/code&gt;" to be the first recipe in a makefile,
since the first recipe is what is built by default
when no other target is passed as an argument.&lt;/p&gt;
&lt;p&gt;Another traditional target is "&lt;code&gt;clean&lt;/code&gt;".
Add the following to your Makefile.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;clean&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    rm --force *.dat *.png zipf_results.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running &lt;code&gt;make clean&lt;/code&gt; will now remove all of the cruft.&lt;/p&gt;
&lt;p&gt;Watch out, though!&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens if you create a file named &lt;code&gt;clean&lt;/code&gt; (i.e. &lt;code&gt;touch clean&lt;/code&gt;)
and then run &lt;code&gt;make clean&lt;/code&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When you run &lt;code&gt;make clean&lt;/code&gt; you get &lt;code&gt;make: Nothing to be done for 'clean'.&lt;/code&gt;.
That's &lt;em&gt;not&lt;/em&gt; because all those files have already been removed.
&lt;em&gt;Make&lt;/em&gt; isn't that smart.
Instead, make sees that there is already a file named "&lt;code&gt;clean&lt;/code&gt;" and,
since this file is newer than all of its prerequisites (there are none),
&lt;em&gt;Make&lt;/em&gt; decides there's nothing left to do.&lt;/p&gt;
&lt;p&gt;To avoid this problem add the following to your Makefile.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;.PHONY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This special target tells &lt;em&gt;Make&lt;/em&gt; to assume that the targets "all", and "clean"
are &lt;em&gt;not&lt;/em&gt; real files;
they're &lt;strong&gt;phony&lt;/strong&gt; targets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add Makefile
git commit -m &lt;span class="s2"&gt;&amp;quot;Added &amp;#39;all&amp;#39; and &amp;#39;clean&amp;#39; recipes.&amp;quot;&lt;/span&gt;
rm clean
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;&lt;em&gt;Make&lt;/em&gt; features&lt;/h1&gt;
&lt;p&gt;(Estimated time: 45 minutes)&lt;/p&gt;
&lt;p&gt;Right now our Makefile looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c"&gt;# Dummy targets&lt;/span&gt;
&lt;span class="nf"&gt;all&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;zipf_results&lt;/span&gt;.&lt;span class="n"&gt;tgz&lt;/span&gt;

&lt;span class="nf"&gt;clean&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    rm --force *.dat *.png zipf_results.tgz

&lt;span class="nf"&gt;.PHONY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt;
&lt;span class="nf"&gt;.SECONDARY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="c"&gt;# Analysis and plotting&lt;/span&gt;
&lt;span class="nf"&gt;isles.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/&lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./wordcount.py books/isles.txt isles.dat

&lt;span class="nf"&gt;isles.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt;
    ./plotcount.py isles.dat isles.png

&lt;span class="nf"&gt;abyss.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/&lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./wordcount.py books/abyss.txt abyss.dat

&lt;span class="nf"&gt;abyss.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt;
    ./plotcount.py abyss.dat abyss.png

&lt;span class="c"&gt;# Archive for sharing&lt;/span&gt;
&lt;span class="nf"&gt;zipf_results.tgz&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt;
    rm -rf zipf_results/
    mkdir zipf_results/
    cp isles.dat abyss.dat isles.png abyss.png zipf_results/
    tar -czf zipf_results.tgz zipf_results/
    rm -r zipf_results/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Looks good, don't you think?
Notice the added comments, starting with the "&lt;code&gt;#&lt;/code&gt;" character just like in
Python, R, shell, etc.&lt;/p&gt;
&lt;p&gt;Using these recipes, a simple call to &lt;code&gt;make&lt;/code&gt; builds all the same files that
we were originally making either manually or using the master script,
but with a few bonus features.&lt;/p&gt;
&lt;p&gt;Now, if we change one of the inputs, we don't have to rebuild everything.
Instead, &lt;em&gt;Make&lt;/em&gt; knows to only rebuild the files that, either directly or
indirectly, depend on the file that changed.
This is called an &lt;strong&gt;incremental build&lt;/strong&gt;.
It's no longer our job to track those dependencies.
One fewer cognitive burden getting in the way of research progress!&lt;/p&gt;
&lt;p&gt;In addition, a makefile explicitly documents the inputs to and outputs
from every step in the analysis.
These are like informal "USAGE:" documentation for our scripts.&lt;/p&gt;
&lt;h2&gt;Parallel &lt;em&gt;Make&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;And check this out!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;make clean
make --jobs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Did you see it?
The &lt;code&gt;--jobs&lt;/code&gt; flag (just "&lt;code&gt;-j&lt;/code&gt;" works too) tells &lt;em&gt;Make&lt;/em&gt; to run recipes in
&lt;em&gt;parallel&lt;/em&gt;.
Our dependency graph clearly shows that
&lt;code&gt;abyss.dat&lt;/code&gt; and &lt;code&gt;isles.dat&lt;/code&gt; are mutually independent and can
both be built at the same time.
Likewise for &lt;code&gt;abyss.png&lt;/code&gt; and &lt;code&gt;isles.png&lt;/code&gt;.
If you've got a bunch of independent branches in your analysis, this can
greatly speed up your build process.&lt;/p&gt;
&lt;h2&gt;D.R.Y. (Don't Repeat Yourself)&lt;/h2&gt;
&lt;p&gt;In many programming language, the bulk of the language features are there
to allow the programmer to describe long-winded computational routines as
short, expressive, beautiful code.
Features in Python or R like user-defined variables and functions are
useful in part because they mean we don't have to write out (or think about)
all of the details over and over again.
This good habit of writing things out only once is known as the D.R.Y.
principle.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;Make&lt;/em&gt; a number of features are designed to minimize repetitive code.
Our current makefile does &lt;em&gt;not&lt;/em&gt; conform to this principle,
but &lt;em&gt;Make&lt;/em&gt; is perfectly capable of doing so.&lt;/p&gt;
&lt;h2&gt;Automatic variables&lt;/h2&gt;
&lt;p&gt;One overly repetitive part of our Makefile:
Targets and prerequisites are in both the header &lt;em&gt;and&lt;/em&gt; the recipe of each rule.&lt;/p&gt;
&lt;p&gt;It turns out, that&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;isles.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/&lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./wordcount.py books/isles.txt isles.dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;can be rewritten as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;isles.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/&lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./wordcount.py $^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we've replaced the prerequisite "&lt;code&gt;books/isles.txt&lt;/code&gt;" in the recipe
with "&lt;code&gt;$^&lt;/code&gt;" and the target "&lt;code&gt;isles.dat&lt;/code&gt;" with "&lt;code&gt;$@&lt;/code&gt;".
Both "&lt;code&gt;$^&lt;/code&gt;" and "&lt;code&gt;$@&lt;/code&gt;" are variables that refer to all of the prerequisites and
target of a rule, respectively.
In &lt;em&gt;Make&lt;/em&gt;, variables are referenced with a leading dollar sign symbol.
While we can also define our own variables,
&lt;em&gt;Make&lt;/em&gt; &lt;em&gt;automatically&lt;/em&gt; defines a number of variables, like the ones
I've just shown you&lt;sup id="fnref:auto-vars"&gt;&lt;a class="footnote-ref" href="#fn:auto-vars"&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Therefore&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;zipf_results.tgz&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt;
    rm -rf zipf_results/
    mkdir zipf_results/
    cp isles.dat abyss.dat isles.png abyss.png zipf_results/
    tar -czf zipf_results.tgz zipf_results/
    rm -r zipf_results/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;can now be rewritten as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;zipf_results.tgz&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;dat&lt;/span&gt; &lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt;
    rm -rf zipf_results/
    mkdir zipf_results/
    cp $^ zipf_results/
    tar -czf &lt;span class="nv"&gt;$@&lt;/span&gt; zipf_results/
    rm -r zipf_results/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's a little less cluttered,
and still perfectly understandable once you know what the variables mean.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;```bash
make clean
make isles.dat
``````````&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--Those extra backticks are because of Vim syntax highlighting.--&gt;

&lt;p&gt;You should get the same output as last time.
Internally, &lt;em&gt;Make&lt;/em&gt; replaced "&lt;code&gt;$@&lt;/code&gt;" with "&lt;code&gt;isles.dat&lt;/code&gt;"
and "&lt;code&gt;$^&lt;/code&gt;" with "&lt;code&gt;books/isles.txt&lt;/code&gt;"
before running the recipe.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Go ahead and rewrite all of the rules in your Makefile to minimize
repetition and take advantage of these automatic variables.
Don't forget to commit your work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Pattern rules&lt;/h2&gt;
&lt;p&gt;Another deviation from D.R.Y.:
We have nearly identical recipes for &lt;code&gt;abyss.dat&lt;/code&gt; and &lt;code&gt;isles.dat&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It turns out we can replace &lt;em&gt;both&lt;/em&gt; of those rules with just one rule,
by telling &lt;em&gt;Make&lt;/em&gt; about the relationships between filename &lt;em&gt;patterns&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A "pattern rule" looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;%.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    countwords.py $^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we've replaced the book name with a percent sign, "&lt;code&gt;%&lt;/code&gt;".
The "&lt;code&gt;%&lt;/code&gt;" is called the &lt;strong&gt;stem&lt;/strong&gt;
and matches any sequence of characters in the target.
(Kind of like a "&lt;code&gt;*&lt;/code&gt;" (glob) in a path name, but they are &lt;em&gt;not&lt;/em&gt; the same.)
Whatever it matches is then filled in to the prerequisites
wherever there's a "&lt;code&gt;%&lt;/code&gt;".&lt;/p&gt;
&lt;p&gt;This rule can be interpreted as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In order to build a file named &lt;code&gt;[something].dat&lt;/code&gt; (the target)
find a file named &lt;code&gt;books/[that same something].txt&lt;/code&gt; (the prerequisite)
and run &lt;code&gt;countwords.py [the prerequisite] [the target]&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notice how helpful the automatic variables are here.
This recipe will work no matter what stem is being matched!&lt;/p&gt;
&lt;p&gt;We can replace &lt;em&gt;both&lt;/em&gt; of the rules that matched this pattern
(&lt;code&gt;abyss.dat&lt;/code&gt; and &lt;code&gt;isles.dat&lt;/code&gt;) with just one rule.
Go ahead and do that in your Makefile.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;After you've replaced the two rules with one pattern
rule, try removing all of the products (&lt;code&gt;make clean&lt;/code&gt;)
and rerunning the pipeline.&lt;/p&gt;
&lt;p&gt;Is anything different now that you're using the pattern rule?&lt;/p&gt;
&lt;p&gt;If everything still works, commit your changes to &lt;em&gt;Git&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Replace the recipes for &lt;code&gt;abyss.png&lt;/code&gt; and &lt;code&gt;isles.png&lt;/code&gt;
with a single pattern rule.&lt;/p&gt;
&lt;h4&gt;Challenge&lt;/h4&gt;
&lt;p&gt;Add &lt;code&gt;books/sierra.txt&lt;/code&gt; to your pipeline.&lt;/p&gt;
&lt;p&gt;(i.e. &lt;code&gt;make all&lt;/code&gt; should plot the word counts and add the plots to
&lt;code&gt;zipf_results.tgz&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Commit your changes to &lt;em&gt;Git&lt;/em&gt; before we move on.&lt;/p&gt;
&lt;h2&gt;User defined variables&lt;/h2&gt;
&lt;p&gt;Not all variables in a makefile are of the automatic variety.
Users can define their own, as well.&lt;/p&gt;
&lt;p&gt;Add this lines at the top of your makefile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;ARCHIVED&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; isles.dat isles.png &lt;span class="se"&gt;\&lt;/span&gt;
            abyss.dat abyss.png &lt;span class="se"&gt;\&lt;/span&gt;
            sierra.dat sierra.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Just like many other languages,
in makefiles "&lt;code&gt;\&lt;/code&gt;" is a line-continuation character.
Think of this variable definition as a single line without the backslash.&lt;/p&gt;
&lt;p&gt;The variable &lt;code&gt;ARCHIVED&lt;/code&gt; is a list of the files that we want to include in our
tarball.
Now wherever we write &lt;code&gt;${ARCHIVED}&lt;/code&gt; it will be replaced with that list of files.
The dollar sign, "&lt;code&gt;$&lt;/code&gt;", and curly-braces, "&lt;code&gt;{}&lt;/code&gt;", are both mandatory when
inserting the contents of a variable.&lt;/p&gt;
&lt;p&gt;Notice the backslashes in the variable definition
splitting the list over three lines, instead of one very long line.
Also notice that we assigned to the variable with "&lt;code&gt;:=&lt;/code&gt;".
This is generally a Good Idea;
Assigning with a normal equals sign can result in non-intuitive behavior for
reasons that we will not be talking about&lt;sup id="fnref:var-assign"&gt;&lt;a class="footnote-ref" href="#fn:var-assign"&gt;5&lt;/a&gt;&lt;/sup&gt;.
Finally, notice that the items in our list are separated by &lt;em&gt;whitespace&lt;/em&gt;,
not commas.
Prerequisite lists were the same way; this is just how lists of things work in
makefiles.
If you included commas they would be considered parts of the filenames.&lt;/p&gt;
&lt;p&gt;Using this variable we can replace the prerequisites of &lt;code&gt;zipf_results.tgz&lt;/code&gt;.
That rule would now be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;zipf_results.tgz&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; ${&lt;span class="n"&gt;ARCHIVED&lt;/span&gt;}
    rm -rf zipf_results/
    mkdir zipf_results/
    cp $^ zipf_results/
    tar -czf &lt;span class="nv"&gt;$@&lt;/span&gt; zipf_results/
    rm -r zipf_results/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can also use &lt;code&gt;${ARCHIVED}&lt;/code&gt; to simplify our cleanup rule.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;clean&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    rm --force &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;ARCHIVED&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt; zipf_results.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;Try running &lt;code&gt;clean&lt;/code&gt; and then &lt;code&gt;all&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Does everything still work?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Best practices for &lt;em&gt;Make&lt;/em&gt;-based projects&lt;/h1&gt;
&lt;p&gt;(Estimated time: 60 minutes)&lt;/p&gt;
&lt;p&gt;A Makefile can be an important part of a reproducible research pipeline.
Have you noticed how simple it is now to add/remove books from our analysis?
Just add or remove those files from the definition of &lt;code&gt;ARCHIVED&lt;/code&gt; or
the prerequisites for the &lt;code&gt;all&lt;/code&gt; target!
With a master script approach, like &lt;code&gt;run_pipeline.sh&lt;/code&gt;,
adding an additional book required either more complicated
or less transparent changes.&lt;/p&gt;
&lt;h2&gt;What's a prerequisite?&lt;/h2&gt;
&lt;p&gt;We've talked a lot about the power of &lt;em&gt;Make&lt;/em&gt; for
rebuilding research outputs when input data changes.
When doing novel data analysis, however, it's very common for our &lt;em&gt;scripts&lt;/em&gt; to
be as or &lt;em&gt;more&lt;/em&gt; dynamic than the data.&lt;/p&gt;
&lt;p&gt;What happens when we edit our scripts instead of changing our data?&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;First, run &lt;code&gt;make all&lt;/code&gt; so your analysis is up-to-date.&lt;/p&gt;
&lt;p&gt;Let's change the default number of entries in the rank/frequency
plot from 10 to 5.&lt;/p&gt;
&lt;p&gt;(Hint: edit the function definition for &lt;code&gt;plot_word_counts&lt;/code&gt; in
&lt;code&gt;plotcounts.py&lt;/code&gt; to read &lt;code&gt;limit=5&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;Now run &lt;code&gt;make all&lt;/code&gt; again.  What happened?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As it stands, we have to run &lt;code&gt;make clean&lt;/code&gt; followed by &lt;code&gt;make all&lt;/code&gt;
to update our analysis with the new script.
We're missing out on the benefits of incremental analysis when our scripts
are changing too.&lt;/p&gt;
&lt;p&gt;There must be a better way...and there is!
&lt;em&gt;Scripts should be prerequisites too.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's edit the pattern rule for &lt;code&gt;%.png&lt;/code&gt; to include &lt;code&gt;plotcounts.py&lt;/code&gt;
as a prerequisites.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;%.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;plotcounts&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; %.&lt;span class="n"&gt;dat&lt;/span&gt;
    ./$^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The header makes sense, but that's a strange looking recipe:
just two automatic variables.&lt;/p&gt;
&lt;p&gt;This recipe works because "&lt;code&gt;$^&lt;/code&gt;" is replaced with all of the prerequisites.
&lt;em&gt;In order&lt;/em&gt;.
When building &lt;code&gt;abyss.png&lt;/code&gt;, for instance, '&lt;code&gt;./$^ $@&lt;/code&gt;' becomes
&lt;code&gt;./plotcounts.py abyss.dat&lt;/code&gt;, which is actually exactly what we want.&lt;/p&gt;
&lt;p&gt;(Remember that we need the leading '&lt;code&gt;./&lt;/code&gt;' so that Bash knows we're executing
a file in the current directory and not a command in our path.)&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;What happens when you run the pipeline after modifying your script again?&lt;/p&gt;
&lt;p&gt;(Changes to your script can be simulated with &lt;code&gt;touch plotcounts.py&lt;/code&gt;.)&lt;/p&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Update your other rules to include the relevant scripts as prerequisites.&lt;/p&gt;
&lt;p&gt;Commit your changes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Directory structure&lt;/h2&gt;
&lt;p&gt;Take a look at all of the clutter in your project directory (run &lt;code&gt;ls&lt;/code&gt; to
list all of the files).
For such a small project that's a lot of junk!
Imagine how hard it would be to find your way around this analysis
if you had more than three steps?
Let's move some stuff around to make our project easier to navigate.&lt;/p&gt;
&lt;h3&gt;Store scripts in &lt;code&gt;scripts/&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;First we'll stow away the scripts.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;mkdir scripts/&lt;/span&gt;
&lt;span class="err"&gt;mv plotcounts.py wordcount.py scripts/&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We also need to update our Makefile to reflect the change:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;%.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;countwords&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    ./$^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;%.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;plotcounts&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; %.&lt;span class="n"&gt;dat&lt;/span&gt;
    ./$^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;becomes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;%.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;countwords&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;%.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;plotcounts&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; %.&lt;span class="n"&gt;dat&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's a little more verbose, but it is now explicit
that &lt;code&gt;countwords.py&lt;/code&gt; and &lt;code&gt;plotcount.py&lt;/code&gt; are scripts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Git&lt;/em&gt; should have no problem with the move once you tell it which files
to be aware of.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add countwords.py plotcounts.py
git add scripts/countwords.py scripts/plotcounts.py
git add Makefile
git commit -m &lt;span class="s2"&gt;&amp;quot;Move scripts into a subdirectory.&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Great!  From here on, when we add new scripts to our analysis they won't
clutter up our project root.&lt;/p&gt;
&lt;h3&gt;"Hide" intermediate files in &lt;code&gt;data/&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Speaking of clutter, what are we gonna do about all of these intermediate files!?
Put 'em in a subdirectory!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mkdir data/
mv *.tsv data/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And then fix up your Makefile.
Adjust the relevant lines to look like this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c"&gt;# ...&lt;/span&gt;

&lt;span class="nv"&gt;ARCHIVED&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; data/isles.dat isles.png &lt;span class="se"&gt;\&lt;/span&gt;
            data/abyss.dat abyss.png &lt;span class="se"&gt;\&lt;/span&gt;
            data/sierra.dat sierra.png

&lt;span class="c"&gt;# ...&lt;/span&gt;

&lt;span class="nf"&gt;data/%.dat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;countwords&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;%.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;plotcounts&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;/%.&lt;span class="n"&gt;dat&lt;/span&gt;

&lt;span class="c"&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Thanks to our &lt;code&gt;ARCHIVED&lt;/code&gt; variable, making these changes is pretty simple.&lt;/p&gt;
&lt;p&gt;We have to make one more change if we don't want &lt;em&gt;Git&lt;/em&gt; to bother us about
untracked files.
Update your &lt;code&gt;.gitignore&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;data/*.dat&lt;/span&gt;
&lt;span class="err"&gt;*.png&lt;/span&gt;
&lt;span class="err"&gt;zipf_results.tgz&lt;/span&gt;
&lt;span class="err"&gt;LICENSE.md&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now commit your changes.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add Makefile
git add .gitignore
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Simple!&lt;/p&gt;
&lt;h3&gt;Output finished products to &lt;code&gt;fig/&lt;/code&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Update your Makefile so that the plots and &lt;code&gt;zipf_results.tgz&lt;/code&gt; are in a
directory called &lt;code&gt;fig/&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can call this directory something else if you prefer, but &lt;code&gt;fig/&lt;/code&gt; seems
short and descriptive.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Try it&lt;/h4&gt;
&lt;p&gt;Does your pipeline still execute the way you expect?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;File naming&lt;/h2&gt;
&lt;h3&gt;Use file extensions to indicate format&lt;/h3&gt;
&lt;p&gt;Up to this point, we've been working with three types of data files,
each with it's own file extension.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"&lt;code&gt;.txt&lt;/code&gt;" files: the original book in plain-text&lt;/li&gt;
&lt;li&gt;"&lt;code&gt;.dat&lt;/code&gt;" files: word counts and percentages in a plain-text format&lt;/li&gt;
&lt;li&gt;"&lt;code&gt;.png&lt;/code&gt;" files: PNG formatted barplots&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using file extensions like these clearly indicates to anyone not familiar with
your project what software to view each file with;
you won't get much out of opening a PNG with a text editor.
Whenever possible, use a widely used extension to make it easy for others
to understand your data.&lt;/p&gt;
&lt;p&gt;File extensions also give us a handle for describing the flow of data in our
pipeline.
Pattern rules rely on this convention.
Our makefile says that the raw, book data feeds into word count data
which feeds into barplot data.&lt;/p&gt;
&lt;p&gt;But the current naming scheme has one obvious ambiguity:
"&lt;code&gt;.dat&lt;/code&gt;" isn't particularly descriptive.
Lots of file formats can be described as "data", including binary formats
that would require specialized software to view.
For tab-delimited, tabular data (data in rows and columns),
"&lt;code&gt;.tsv&lt;/code&gt;" is a more precise convention.&lt;/p&gt;
&lt;p&gt;Updating our pipeline to use this extension is as simple as find-and-replace
"&lt;code&gt;.dat&lt;/code&gt;" to "&lt;code&gt;.tsv&lt;/code&gt;" in our Makefile.
If you're tired of &lt;code&gt;mv&lt;/code&gt;-ing your files every time you change your pipeline
you can also &lt;code&gt;make clean&lt;/code&gt; followed by &lt;code&gt;make all&lt;/code&gt; to check that everything still
works.&lt;/p&gt;
&lt;p&gt;You might want to update your "&lt;code&gt;clean&lt;/code&gt;" recipe to remove all the junk
like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;clean&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    rm -f data/* fig/*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Be sure to commit all of your changes.&lt;/p&gt;
&lt;h3&gt;Infix processing hints&lt;/h3&gt;
&lt;p&gt;One of our goals in implementing best practices for our analysis pipeline
is to make it easy to change it without rewriting everything.
Let's add a preprocessing step to our analysis that puts
everything in lowercase before counting words.&lt;/p&gt;
&lt;p&gt;The program &lt;code&gt;tr&lt;/code&gt; (short for "translate") is a Unix-style filter that swaps one
set of characters for another.
&lt;code&gt;tr '[:upper:]' '[:lower:]' &amp;lt; [input file] &amp;gt; [output file]&lt;/code&gt;
will read the mixedcase input file and write all lowercase to
the output file.&lt;/p&gt;
&lt;p&gt;We can add this to our pipeline.
We know the recipe is going to look like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;tr &amp;#39;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;:]&amp;#39; &amp;#39;[:&lt;span class="n"&gt;lower&lt;/span&gt;:]&amp;#39; &amp;lt; $^ &amp;gt; $@
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
&lt;h4&gt;Challenge&lt;/h4&gt;
&lt;p&gt;Rewrite your Makefile to update the pipeline with the preprocessing step.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You probably decided to take the pattern &lt;code&gt;books/%.txt&lt;/code&gt; as the prerequisite,
but what did you opt to name the target?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data/%.txt&lt;/code&gt; is an option, but that means we have two files named
&lt;code&gt;[bookname].txt&lt;/code&gt;, one in &lt;code&gt;books/&lt;/code&gt; and one in &lt;code&gt;data/&lt;/code&gt;.
Probably not the easiest to differentiate.&lt;/p&gt;
&lt;p&gt;A better option is to use a more descriptive filename.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;data/%.lower.txt&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    tr &lt;span class="s1"&gt;&amp;#39;[:upper:]&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;[:lower:]&amp;#39;&lt;/span&gt; &amp;lt; $^ &amp;gt; &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;By including an &lt;strong&gt;infix&lt;/strong&gt; of &lt;code&gt;.lower.&lt;/code&gt; in our filename it's easy to
see that one file is a lowercase version of the mixedcase original.
Now we can extend our pipeline with a variety of pre- and post-processing
steps, give each of them a descriptive infix,
and the names will be a self-documenting record of its origins.&lt;/p&gt;
&lt;p&gt;For reasons which may be apparent in a minute, let's also make a dummy
preprocessing step which will just copy the books verbatim into our
&lt;code&gt;data/&lt;/code&gt; directory.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;data/%.txt&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    cp $^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And, in the spirit of infixes, we'll rename &lt;code&gt;data/%.tsv&lt;/code&gt; to be more descriptive.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;data/%.counts.tsv&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;wordcount&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;fig/%.counts.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;plotcount&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;/%.&lt;span class="n"&gt;counts&lt;/span&gt;.&lt;span class="n"&gt;tsv&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Our Makefile now documents explicitly that we require a tab-delimited table of
word counts in order to generate the plot,
whereas before any &lt;code&gt;.tsv&lt;/code&gt; file would suggest a word-plot was possible.&lt;/p&gt;
&lt;p&gt;Here's the &lt;em&gt;full&lt;/em&gt; Makefile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;ARCHIVED&lt;/span&gt; &lt;span class="o"&gt;:=&lt;/span&gt; data/isles.lower.counts.tsv data/abyss.lower.counts.tsv &lt;span class="se"&gt;\&lt;/span&gt;
            data/sierra.lower.counts.tsv fig/isles.lower.counts.png &lt;span class="se"&gt;\&lt;/span&gt;
            fig/abyss.lower.counts.png fig/sierra.lower.counts.png

&lt;span class="c"&gt;# Dummy targets&lt;/span&gt;
&lt;span class="nf"&gt;all&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;/&lt;span class="n"&gt;isles&lt;/span&gt;.&lt;span class="n"&gt;lower&lt;/span&gt;.&lt;span class="n"&gt;counts&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;/&lt;span class="n"&gt;abyss&lt;/span&gt;.&lt;span class="n"&gt;lower&lt;/span&gt;.&lt;span class="n"&gt;counts&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; \
        &lt;span class="n"&gt;fig&lt;/span&gt;/&lt;span class="n"&gt;sierra&lt;/span&gt;.&lt;span class="n"&gt;lower&lt;/span&gt;.&lt;span class="n"&gt;counts&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt; &lt;span class="n"&gt;zipf_results&lt;/span&gt;.&lt;span class="n"&gt;tgz&lt;/span&gt;

&lt;span class="nf"&gt;clean&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    rm --force data/* fig/*

&lt;span class="nf"&gt;.PHONY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt;
&lt;span class="nf"&gt;.SECONDARY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="c"&gt;# Analysis and plotting&lt;/span&gt;
&lt;span class="nf"&gt;data/%.txt&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;books&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    cp $^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;data/%.lower.txt&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    tr &lt;span class="s1"&gt;&amp;#39;[:upper:]&amp;#39;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;[:lower:]&amp;#39;&lt;/span&gt; &amp;lt; $^ &amp;gt; &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;data/%.counts.tsv&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;wordcount&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;/%.&lt;span class="n"&gt;txt&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="nf"&gt;fig/%.counts.png&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scripts&lt;/span&gt;/&lt;span class="n"&gt;plotcount&lt;/span&gt;.&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;/%.&lt;span class="n"&gt;counts&lt;/span&gt;.&lt;span class="n"&gt;tsv&lt;/span&gt;
    $^ &lt;span class="nv"&gt;$@&lt;/span&gt;

&lt;span class="c"&gt;# Archive for sharing&lt;/span&gt;
&lt;span class="nf"&gt;zipf_results.tgz&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; ${&lt;span class="n"&gt;ARCHIVED&lt;/span&gt;}
    rm -rf zipf_results/
    mkdir zipf_results/
    cp $^ zipf_results/
    tar -czf &lt;span class="nv"&gt;$@&lt;/span&gt; zipf_results/
    rm -r zipf_results/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Our filenames are certainly more verbose now, but in exchange we get:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;self-documenting filenames&lt;/li&gt;
&lt;li&gt;more flexible development&lt;/li&gt;
&lt;li&gt;and something else, too...&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;make clean
make fig/abyss.lower.counts.png
make fig/abyss.counts.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What happened there?
We just built two different barplots, one for our analysis &lt;em&gt;with&lt;/em&gt; the
preprocessing step and one &lt;em&gt;without&lt;/em&gt;.
Both from the same Makefile.
By liberally applying pattern rules and infix filenames
we get something like a "filename language".
We describe the analyses we want to run and then have &lt;em&gt;Make&lt;/em&gt; figure out the
details.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Update your drawing of the dependency graph.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Built-in Testing&lt;/h2&gt;
&lt;p&gt;It's a Good Idea to check your analysis against some form of ground truth.
The simplest version of this is a well-defined dataset that you can
reason about independent of your code.
Let's make just such a dataset.
Let's write a book!&lt;/p&gt;
&lt;p&gt;Into a file called &lt;code&gt;books/test.txt&lt;/code&gt; add something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="n"&gt;Book&lt;/span&gt;
&lt;span class="k"&gt;By&lt;/span&gt; &lt;span class="n"&gt;Me&lt;/span&gt;

&lt;span class="n"&gt;This&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;book&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;wrote&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We don't need software to count all of the words in this book, and
we can probably imagine exactly what a barplot of the count would look like.
If the actual result doesn't look like we expected,
then there's probably something wrong with our analysis.
Testing your scripts with this tiny book is computationally cheap, too.&lt;/p&gt;
&lt;p&gt;Let's try it out!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;make fig/test.lower.counts.png
less data/test.lower.counts.tsv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Does your counts data match what you expected?&lt;/p&gt;
&lt;p&gt;We should run this test for just about every change we make,
to our scripts or to our Makefile.
We're going to do that a &lt;em&gt;lot&lt;/em&gt; so we'll make it as easy as possible.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;test&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;/&lt;span class="n"&gt;test&lt;/span&gt;.&lt;span class="n"&gt;lower&lt;/span&gt;.&lt;span class="n"&gt;counts&lt;/span&gt;.&lt;span class="n"&gt;png&lt;/span&gt;

&lt;span class="nf"&gt;.PHONY&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You could even add the &lt;code&gt;test&lt;/code&gt; phony target as the first thing in your Makefile.
That way just calling &lt;code&gt;make&lt;/code&gt; will run your tests.&lt;/p&gt;
&lt;blockquote&gt;
&lt;h4&gt;Practice&lt;/h4&gt;
&lt;p&gt;Add a cleanup target called &lt;code&gt;testclean&lt;/code&gt; which is specific for
the outputs of your test run.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Commit your changes, including &lt;code&gt;books/test.txt&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git add Makefile
git add -f books/test.txt
git commit -m &lt;span class="s2"&gt;&amp;quot;Add pipeline testing recipe and book.&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Review: version control&lt;/h2&gt;
&lt;p&gt;We have been following three guiding principles in our use of version
control during this lesson.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use it (always).&lt;/p&gt;
&lt;p&gt;Version control is a Good Idea and should be used for any files which
describe your pipeline.
This includes notes/documentation/TODOs, scripts, and the Makefiles
themselves.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Don't version control raw or processed data which can be recreated.&lt;/p&gt;
&lt;p&gt;Raw data stays raw and data cleanup should be part of the pipeline.
Because of this, backing up your data is imperative, but version
control is not usually the best way to do so.
Consider adding a recipe which downloads raw data using
&lt;code&gt;wget&lt;/code&gt; or &lt;code&gt;curl&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One exception would be test or example data.
These should be version controlled, as they are subject to change
as testing is adapted to the evolving pipeline.&lt;/p&gt;
&lt;p&gt;In many cases metadata &lt;em&gt;should&lt;/em&gt; be version controlled, since the format
and composition of the metadata is intimately linked with the analysis
pipeline itself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aim to commit "atomic" changes to your pipeline.&lt;/p&gt;
&lt;p&gt;This means you should usually run &lt;code&gt;make test&lt;/code&gt; before committing
your changes so that regressions don't need to be fixed
in subsequent commits.
Co-dependent updates to metadata, documentation, and testing should
be included in the same commit.
In a perfect world, &lt;code&gt;make all&lt;/code&gt; should work, and documentation
should be up to date, regardless of what revision has been checked out.
Excessive application of this principle is ill advised.&lt;/p&gt;
&lt;p&gt;A more common problem are behemoth commits which make large numbers of
unrelated changes.
In general, a single sentence commit message should be able to summarize
all of the changes in a commit.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:similar-title"&gt;
&lt;p&gt;I swear I didn't know about that post when I titled my tutorial.
Great minds think alike?&amp;#160;&lt;a class="footnote-backref" href="#fnref:similar-title" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:makefile-name"&gt;
&lt;p&gt;While several other filenames will work, it is a Good Idea to
always call your Makefile &lt;code&gt;Makefile&lt;/code&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:makefile-name" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:makefile-identification"&gt;
&lt;p&gt;Notice that we didn't tell &lt;em&gt;Make&lt;/em&gt; to use
&lt;code&gt;Makefile&lt;/code&gt;.  When you run &lt;code&gt;make&lt;/code&gt;, the program automatically looks in
several places for your Makefile.&amp;#160;&lt;a class="footnote-backref" href="#fnref:makefile-identification" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:auto-vars"&gt;
&lt;p&gt;See https://www.gnu.org/software/make/manual/html_node/Automatic-Variables.html].&amp;#160;&lt;a class="footnote-backref" href="#fnref:auto-vars" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:var-assign"&gt;
&lt;p&gt;Variables are complicated in &lt;em&gt;Make&lt;/em&gt;.
Read the extensive &lt;a href="https://www.gnu.org/software/make/manual/html_node/Using-Variables.html"&gt;documentation&lt;/a&gt; about variable assignment.&amp;#160;&lt;a class="footnote-backref" href="#fnref:var-assign" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Computing"></category><category term="software-carpentry"></category><category term="teaching"></category><category term="programming"></category><category term="make"></category><category term="pipelines"></category><category term="bioinformatics"></category><category term="software"></category></entry><entry><title>First time teaching Python to novices</title><link href="//blog.byronjsmith.com/swc-python-lesson.html" rel="alternate"></link><published>2015-08-12T01:00:00-04:00</published><updated>2015-08-14T10:00:00-04:00</updated><author><name>Byron J. Smith</name></author><id>tag:blog.byronjsmith.com,2015-08-12:/swc-python-lesson.html</id><summary type="html">&lt;p&gt;This July I co-instructed with &lt;a href="https://impactstory.org/JenniferShelton"&gt;Jennifer Shelton&lt;/a&gt;
a Software Carpentry &lt;a href="http://i5k-kinbre-script-share.github.io/2015-07-23-stanford/"&gt;workshop&lt;/a&gt; at Stanford University,
targeted to researchers with genomic or evolutionary datasets.
Jennifer taught the shell (Bash) and version control with Git,
while I taught the general programming language Python.
I've been aware of the &lt;a href="http://software-carpentry.org/"&gt;organization&lt;/a&gt;, which teaches software
development and computational methods to scientists, since attending a
workshop in 2012.
Since then I've served as a helper at one workshop
(troubleshooting individual learner's problems and helping catch them up with
the rest of the class),
and gone through the "accelerated", two day, instructor training at Michigan
State University.
After the Stanford workshop, I took part in new-instructor debriefing
on August 4th, during which I mentioned that I had to greatly pare down the
community-written lesson plan, &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/"&gt;python-novice-inflammation&lt;/a&gt;,
to fit into the two half-day session we allotted it.&lt;/p&gt;
&lt;p&gt;Karin and Tiffany, who were running the debriefing, asked me to send â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;This July I co-instructed with &lt;a href="https://impactstory.org/JenniferShelton"&gt;Jennifer Shelton&lt;/a&gt;
a Software Carpentry &lt;a href="http://i5k-kinbre-script-share.github.io/2015-07-23-stanford/"&gt;workshop&lt;/a&gt; at Stanford University,
targeted to researchers with genomic or evolutionary datasets.
Jennifer taught the shell (Bash) and version control with Git,
while I taught the general programming language Python.
I've been aware of the &lt;a href="http://software-carpentry.org/"&gt;organization&lt;/a&gt;, which teaches software
development and computational methods to scientists, since attending a
workshop in 2012.
Since then I've served as a helper at one workshop
(troubleshooting individual learner's problems and helping catch them up with
the rest of the class),
and gone through the "accelerated", two day, instructor training at Michigan
State University.
After the Stanford workshop, I took part in new-instructor debriefing
on August 4th, during which I mentioned that I had to greatly pare down the
community-written lesson plan, &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/"&gt;python-novice-inflammation&lt;/a&gt;,
to fit into the two half-day session we allotted it.&lt;/p&gt;
&lt;p&gt;Karin and Tiffany, who were running the debriefing, asked me to send a note
to the mentorship email list about which parts I removed and which I kept in.
I thought I'd also take the opportunity to comment on the material at large:
what worked for me and what didn't.
What started as an email quickly ballooned into this blog post.&lt;/p&gt;
&lt;p&gt;To be explicit, I was teaching from the state of the repository at the time of
the workshop&lt;sup id="fnref:repo-state"&gt;&lt;a class="footnote-ref" href="#fn:repo-state"&gt;1&lt;/a&gt;&lt;/sup&gt; .&lt;/p&gt;
&lt;p&gt;With this as my first workshop&lt;sup id="fnref:unprepared"&gt;&lt;a class="footnote-ref" href="#fn:unprepared"&gt;2&lt;/a&gt;&lt;/sup&gt;, I (incorrectly) thought
I could teach all of the topics straight through.
By the time it became apparent that this wasn't going to work,
adapting the first day's material had to be done on the fly.
After that experience, and
before the following afternoon,
I prepared a subset of the remaining material that I thought I could cover.
I'm now relying on my (somewhat traumatic) memory of the first session,
and that outline I put together for the second day to write this summary.&lt;/p&gt;
&lt;p&gt;My plan going in was to split &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/index.html#topics"&gt;the material&lt;/a&gt; after Topic 6,
getting learners up to writing functions on the first day,
so that we could discuss debugging and best-practices,
and transition from the Jupyter notebook to shell scripts, the next day.
Based on my co-instructors recommendation,
I did not have learners do all of the challenge questions for each topic,
but instead picked just one or two that I thought would be most useful.&lt;/p&gt;
&lt;p&gt;I found myself wishing (especially for Topic 1: "Analyzing Patient Data") that
some of the easier questions were integrated into the lesson itself, instead of
all at the bottom.
Learners should have had more chances to problem-solve early, instead of
listening to me for the entirety of each topic before getting their feet wet.&lt;/p&gt;
&lt;h2&gt;Motivating Python&lt;/h2&gt;
&lt;p&gt;For that &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/01-numpy.html"&gt;first topic&lt;/a&gt; I &lt;em&gt;did&lt;/em&gt; cover everything, but wish I hadn't,
since it was mostly focused on array operations and the specifics of working
with NumPy (e.g. operations along axes).
I appreciated that we were showing the learners powerful library features to
motivate the later work, but I didn't feel like it was great for this
workshop's "genomics" audience.
Maybe these initial motivating sections should be targeted the same way the
capstone projects are.
It was also too long relative to the other sections, in my opinion.&lt;/p&gt;
&lt;p&gt;It &lt;em&gt;was&lt;/em&gt; very good, however, for introducing some python specifics, especially
things that learners coming from other languages like R or Mathematica might
not know (e.g. 0-indexing, slices, that variable assignment happens when each
line is executed, etc.).
It gave learners a chance to be surprised by their misconceptions and ask
questions.
We should do more of that.&lt;/p&gt;
&lt;p&gt;It would have been helpful for the lesson to have pre-built explanations for
0-indexing and right-exclusive slicing, since these were the hard parts and I'm
not happy with the explanations I initially used.&lt;/p&gt;
&lt;p&gt;I found the nature of the made-up data (maximum values smooth and minimum
values as a step-function along the first axis) distracting.
I also didn't know what they were supposed to represent (beyond inflammation
over time), so the "actually doing science" part of the motivation was a bit
lost.
Is there a reason we use these data?&lt;/p&gt;
&lt;h2&gt;Python basics: lists, loops, conditionals, etc.&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://swcarpentry.github.io/python-novice-inflammation/02-loop.html"&gt;Topics 2&lt;/a&gt; and &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/03-lists.html"&gt;3&lt;/a&gt;, "Repeating Actions with Loops" and
"Storing Multiple Values in Lists" respectively, were good and short.
I didn't feel like I had to cut anything out.
However, for-loop syntax was not explicitly covered early in the lesson plan.
It wasn't until I realized I had gotten ahead of myself that we talked about
loop variables, iterables&lt;sup id="fnref:iterables"&gt;&lt;a class="footnote-ref" href="#fn:iterables"&gt;3&lt;/a&gt;&lt;/sup&gt;, and the indented code-block.&lt;/p&gt;
&lt;p&gt;I also thought the segue from Topic 1 to 2 was a bit weak.
This was a theme throughout, mixing the inflammation data with much simpler
stuff (e.g. looping over short strings and lists).
I realize we want to keep the motivation going, but, as a first-time
instructor, I found it to be distracting, and didn't know which I should be
emphasizing to the learners.&lt;/p&gt;
&lt;p&gt;I also picked the wrong challenge question from Topic 1 (reverse &lt;code&gt;'Newton'&lt;/code&gt;
using a loop), since we hadn't covered &lt;code&gt;range&lt;/code&gt;, &lt;code&gt;append&lt;/code&gt;ing to lists,
&lt;code&gt;''.join&lt;/code&gt;, etc.
What novice audience is that question appropriate for?
Maybe the solution is simple and I'm just confused...&lt;/p&gt;
&lt;p&gt;The material for &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/04-files.html"&gt;topic 4&lt;/a&gt;, "Analyzing Data from Multiple Files"
worked well overall.
The only mistake I remember was copy-pasting the big chunk of code from the
lesson (looping over files and drawing sets of plots) instead of typing it out.
I figured since most of the code was library calls, learners wouldn't get
anything out of me taking the time to type all of it.
That may have been true, but it meant the learners weren't executing the code
at the
same time as me, which interrupted the flow of the lesson.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://swcarpentry.github.io/python-novice-inflammation/05-cond.html"&gt;Topic 5&lt;/a&gt;, "Making Choice" (if-statements), was where things got
hairy.
I panicked a bit and went mostly off the lesson plan.
It did not go well.
When I tried to find something in the lesson to get me back on track,
I wished there was more explicit discussion of syntax and booleans.
I was able to review the topic the next day, which I think got any lost
learners
mostly caught up.&lt;/p&gt;
&lt;p&gt;As you can imagine, at this point we were nearing the end of the first day.
I did manage to show the learners the syntax for defining and using functions,
but I covered &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/06-func.html"&gt;topic 6&lt;/a&gt;, "Creating Functions", in its entirety at the
start of the next
session.&lt;/p&gt;
&lt;h2&gt;Learning my lesson&lt;/h2&gt;
&lt;p&gt;After the harrowing experience with conditionals on the first day, I took the
time to write out a personalized lesson outline for the next day with learning
objectives, steps in explaining difficult concepts, and pre-picked
understanding/challenge questions.
The exercise of writing an outline of learning objectives before the class was
very helpful, and something I intend to repeat before future workshops.&lt;/p&gt;
&lt;p&gt;If I remember correctly&lt;sup id="fnref:metamemory"&gt;&lt;a class="footnote-ref" href="#fn:metamemory"&gt;4&lt;/a&gt;&lt;/sup&gt;, the second day I started once again with
functions, and largely based the lesson on the material in
&lt;a href="http://swcarpentry.github.io/python-novice-inflammation/06-func.html"&gt;the topic&lt;/a&gt;.
The temperature conversion formulas were an effective motivator for this
lesson.
I wonder if simple examples, like this one, can replace the more complex
(and, admittedly, more impressive)
inflammation tutorial to demonstrate the value of Python for scientists.
I also integrated material from the &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/07-errors.html"&gt;topic on errors and exceptions&lt;/a&gt;:
tracebacks, syntax errors, etc.
In this combined topic I did not use the &lt;code&gt;import errors_01&lt;/code&gt; example.
It was unclear to me why the lesson plan, as written, uses a black-box script
like &lt;code&gt;errors_01.py&lt;/code&gt;, and not something more explicit, like an index or
attribute error, to dissect the traceback.
I think the explicit approach worked well for the learners in this workshop.
Since we were covering functions anyway, it wasn't hard to get a multi-level
traceback.
Syntax errors also combined nicely with learning function definition syntax.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The author dissecting an attribute error.5" src="//blog.byronjsmith.com/static/images/swc-stanford-byron.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Somewhere in the process of talking about functions we got sidetracked with
&lt;code&gt;open()&lt;/code&gt;.
I was surprised to see that the lesson plans have only limited discussion of
file objects, only really dealing with them in the section on &lt;code&gt;IOErrors&lt;/code&gt;.
I think learners appreciated a chance to see how the array data they had used
the day before were saved as a CSV,
and how they could access the data directly.
It also gave us a chance to show that other objects besides lists and strings
can serve as iterators in for-loops.&lt;/p&gt;
&lt;p&gt;I liked how the topic 6 &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/06-func.html#defining-defaults"&gt;lesson plan&lt;/a&gt; used the library
function &lt;code&gt;numpy.loadtxt()&lt;/code&gt; to talk about default arguments and the &lt;code&gt;help()&lt;/code&gt;
built-in.
I jumped back and forth between examining that function and implementing
the same things (keywords, documentation) in a &lt;code&gt;center()&lt;/code&gt; function we were
building.
The realized lesson was very similar to the repository's lesson plan,
but a little more integrated with errors and exceptions.&lt;/p&gt;
&lt;p&gt;I had the learners implement &lt;code&gt;rescale()&lt;/code&gt; as a challenge question.
We then worked together as a class to add lower and upper bounds.
This was a much more difficult task than I expected
(even just deriving the correct formula),
and served nicely to demonstrate defensive programming and debugging.
While we touched on many of the concepts in &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/08-defensive.html"&gt;topics 8&lt;/a&gt; and
&lt;a href="http://swcarpentry.github.io/python-novice-inflammation/09-debugging.html"&gt;9&lt;/a&gt;,
these ideas, were spread throughout,
and I did not walk through either as an atomic lesson.&lt;/p&gt;
&lt;p&gt;My ultimate goal on the second day was to write a program to calculate
the mean inflammation of each subject in the example files and then
transform the program into a command-line script that would operate as a
UNIX-style filter.
I remember Greg Wilson teaching Python scripting (along with Bash and SQL)
that way during my first workshop (as a &lt;em&gt;learner&lt;/em&gt;!) at MSU
in May 2012&lt;sup id="fnref:swc-msu"&gt;&lt;a class="footnote-ref" href="#fn:swc-msu"&gt;6&lt;/a&gt;&lt;/sup&gt;.
This &lt;a href="http://swcarpentry.github.io/python-novice-inflammation/10-cmdline.html"&gt;last topic&lt;/a&gt; seemed like a worthwhile mini-capstone,
since it would reintroduce ideas from the Bash lesson the day before,
and we could version-control our work with git.
While we managed to run our code as a script (rather than a cell in the
Jupyter notebook), the transition was a little rough around the edges, and we
didn't have time to add &lt;code&gt;sys.argv&lt;/code&gt; or &lt;code&gt;sys.stdin&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Take-aways&lt;/h2&gt;
&lt;p&gt;The second day of Python was much smoother than the first, and, while we
did not get to all of the material, I was satisfied with what we did cover.
It's quite remarkable that learners can go all the way from indexing into lists
to defensive programming and unit tests in just a few hours.
I'm not convinced that we got them far enough to jump right into using Python
for their own work, but I hope it was a good kick-start towards that goal.
I'm amazed some novice workshops only allocate a half-day session to the
programming language (be it Python, R, or Matlab),
although a quick survey of &lt;a href="http://software-carpentry.org/workshops/index.html#future"&gt;upcoming workshops&lt;/a&gt; suggests that almost
&lt;em&gt;all&lt;/em&gt; of them do in fact use two sessions.
Is this the recommended approach (and if so where is it documented)
or have many instructors all independently come to the same conclusion?&lt;/p&gt;
&lt;p&gt;Even so, there's still more material in python-novice-inflammation
than can be covered in two sessions.
I'm under the impression that the repository is sort of &lt;em&gt;meant&lt;/em&gt; to be like
that: way too big, so that instructors can pick and choose the parts that are
most salient for their audience.
This seems like a good idea, but
it was not sufficiently communicated to me as a first-time instructor,
and, while many of the difficulties I had could have been solved with more
comprehensive preparation,
having a "default" subset would have been helpful.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:repo-state"&gt;
&lt;p&gt;&lt;a href="https://github.com/swcarpentry/python-novice-inflammation/tree/76e3ea24406e4b8d684c9b45f3c5fd33e23ac71a"&gt;&lt;code&gt;76e3ea24406e4b8d684c9b45f3c5fd33e23ac71a&lt;/code&gt;&lt;/a&gt;: still the
HEAD as of this writing.&amp;#160;&lt;a class="footnote-backref" href="#fnref:repo-state" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:unprepared"&gt;
&lt;p&gt;and being insufficiently prepared&amp;#160;&lt;a class="footnote-backref" href="#fnref:unprepared" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:iterables"&gt;
&lt;p&gt;Actually, we talked about getting values from lists and how
strings are like lists, rather than about iterables in general.&amp;#160;&lt;a class="footnote-backref" href="#fnref:iterables" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:metamemory"&gt;
&lt;p&gt;Despite the fact that I have those notes, I actually don't
remember the details of that day's lesson as well.
I wonder if there's some weird metamemory thing going on
e.g. &lt;a href="http://www.sciencemag.org/content/333/6043/776.abstract"&gt;this&lt;/a&gt; (unfortunately paywalled).&amp;#160;&lt;a class="footnote-backref" href="#fnref:metamemory" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:photo-credit"&gt;
&lt;p&gt;Photo credit: Amy Hodge (&lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC-BY&lt;/a&gt;)&amp;#160;&lt;a class="footnote-backref" href="#fnref:photo-credit" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:swc-msu"&gt;
&lt;p&gt;The site for this historic event can still be found
&lt;a href="https://web.archive.org/web/20120514195748/http://software-carpentry.org/boot-camps/michigan-state-university-may-2012/"&gt;here&lt;/a&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:swc-msu" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Education"></category><category term="software-carpentry"></category><category term="teaching"></category><category term="programming"></category><category term="mistakes"></category><category term="python"></category></entry></feed>